{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wonjoon_LAB\\PycharmProjects\\AIGS538_Carplate_letter_sequence_geneneration\\venv\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Wonjoon_LAB\\PycharmProjects\\AIGS538_Carplate_letter_sequence_geneneration\\venv\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, CenterCrop, Grayscale\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import dataset as data\n",
    "import model as md\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} device\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "Batch_size = 128\n",
    "Optimizer_type = 'ADAM'\n",
    "Learning_rate = 1e-4\n",
    "Weight_decay = 0\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Sample_visualize(dataloader) :\n",
    "  ## Visualize some preprocess images\n",
    "  ##\n",
    "  ## Input : dataloader of image dataset\n",
    "  ## Output : image plots\n",
    "\n",
    " dataiter = iter(dataloader)\n",
    " images, labels = next(dataiter)\n",
    " # images = images.numpy()\n",
    " for i in range (2) :\n",
    "   plt.subplot(1,4,i+1)\n",
    "   plt.imshow(images[i].reshape((100, 100)), cmap='gray')\n",
    "   plt.xticks([])\n",
    "   plt.yticks([])\n",
    "   print(labels[i])\n",
    "\n",
    "# def visualize_dataset(dataset):\n",
    "#   ## Visualize some preprocess images\n",
    "#   ##\n",
    "#   ## Input : DATASET of image dataset\n",
    "#   ## Output : image plots\n",
    "#   images, labels = dataset[0]\n",
    "#   print(images.size())\n",
    "#   images = images.numpy()\n",
    "#   plt.imshow(images.reshape((64, 192)), cmap='gray')\n",
    "#   plt.xticks([])\n",
    "#   plt.yticks([])\n",
    "#   print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform_set = Compose([Resize((100, 100)),\n",
    "                Grayscale(),\n",
    "                ToTensor(),\n",
    "                Normalize((0.5), (0.5))])\n",
    "train_data = datasets.ImageFolder(root = \"./CNN_letter_dataset\", transform=transform_set)\n",
    "val_data = datasets.ImageFolder(root = \"./CNN_letter_dataset_val\", transform=transform_set)\n",
    "test_data = datasets.ImageFolder(root = \"./CNN_letter_dataset_test\", transform=transform_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=Batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=Batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=Batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n",
      "tensor(17)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize_dataset(full_dataset)\n",
    "Sample_visualize(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 48, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(48, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Flatten(start_dim=1, end_dim=-1)\n",
      "    (13): Linear(in_features=9216, out_features=100, bias=True)\n",
      "    (14): ReLU()\n",
      "    (15): Linear(in_features=100, out_features=6, bias=True)\n",
      "    (16): ReLU()\n",
      "  )\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (output0): Sequential(\n",
      "    (0): Linear(in_features=4608, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=36, bias=True)\n",
      "    (5): Softmax(dim=1)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=90, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = md.CNN(init_weights=True)\n",
    "model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# import torchsummary\n",
    "#\n",
    "# torchsummary.summary(model,input_size=(1,128,128))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "if Optimizer_type == 'ADAM' :\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr = Learning_rate, weight_decay = Weight_decay)\n",
    "elif Optimizer_type == 'SGD' :\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr = Learning_rate, weight_decay = Weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "## Training function ##\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "  for batch_idx, (X,y) in enumerate(dataloader):\n",
    "\n",
    "    X = X.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_idx % 10 == 0:\n",
    "      print('loss: {:.6f} [{}/{}]'.format(loss.item(), batch_idx*len(X), len(dataloader.dataset)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "## Validation function ##\n",
    "def validation(model, valdata):\n",
    "  ## Input : trained model, validation data\n",
    "  ## Output : validation loss\n",
    "\n",
    "  model.eval()\n",
    "  val_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in valdata:\n",
    "      X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "      output = model(X)\n",
    "      val_loss += nn.functional.cross_entropy(output, y, reduction='sum').item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "  val_loss /= len(valdata.dataset)\n",
    "  print('\\n***Validation Result***\\nAverage loss: {:.6f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(val_loss, correct, len(valdata.dataset), 100*correct/len(valdata.dataset)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def test(model, testdata):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in testdata:\n",
    "      X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "      output = model(X)\n",
    "      test_loss += nn.functional.cross_entropy(output, y, reduction='sum').item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "  test_loss /= len(testdata.dataset)\n",
    "  print('\\n***Test Result***\\nAverage loss: {:.6f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(test_loss, correct, len(testdata.dataset), 100*correct/len(testdata.dataset)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1]\n",
      "loss: 3.583439 [0/28137]\n",
      "loss: 3.581081 [1280/28137]\n",
      "loss: 3.574831 [2560/28137]\n",
      "loss: 3.558127 [3840/28137]\n",
      "loss: 3.536454 [5120/28137]\n",
      "loss: 3.447035 [6400/28137]\n",
      "loss: 3.392435 [7680/28137]\n",
      "loss: 3.278379 [8960/28137]\n",
      "loss: 3.164837 [10240/28137]\n",
      "loss: 3.225582 [11520/28137]\n",
      "loss: 3.033533 [12800/28137]\n",
      "loss: 3.084547 [14080/28137]\n",
      "loss: 3.026021 [15360/28137]\n",
      "loss: 3.039042 [16640/28137]\n",
      "loss: 2.978926 [17920/28137]\n",
      "loss: 2.953110 [19200/28137]\n",
      "loss: 2.950624 [20480/28137]\n",
      "loss: 2.979708 [21760/28137]\n",
      "loss: 2.929191 [23040/28137]\n",
      "loss: 2.904306 [24320/28137]\n",
      "loss: 2.883046 [25600/28137]\n",
      "loss: 2.928840 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.921624, Accuracy: 2703/3725 (72.6%)\n",
      "\n",
      "\n",
      "[Epoch 2]\n",
      "loss: 2.984024 [0/28137]\n",
      "loss: 3.015886 [1280/28137]\n",
      "loss: 2.926834 [2560/28137]\n",
      "loss: 2.889938 [3840/28137]\n",
      "loss: 2.894942 [5120/28137]\n",
      "loss: 2.797803 [6400/28137]\n",
      "loss: 2.893865 [7680/28137]\n",
      "loss: 2.949132 [8960/28137]\n",
      "loss: 2.861302 [10240/28137]\n",
      "loss: 2.873922 [11520/28137]\n",
      "loss: 2.920134 [12800/28137]\n",
      "loss: 2.890190 [14080/28137]\n",
      "loss: 2.844248 [15360/28137]\n",
      "loss: 2.935619 [16640/28137]\n",
      "loss: 2.912015 [17920/28137]\n",
      "loss: 2.884065 [19200/28137]\n",
      "loss: 2.940644 [20480/28137]\n",
      "loss: 2.863686 [21760/28137]\n",
      "loss: 2.904090 [23040/28137]\n",
      "loss: 2.888244 [24320/28137]\n",
      "loss: 2.871421 [25600/28137]\n",
      "loss: 2.880141 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.889754, Accuracy: 2760/3725 (74.1%)\n",
      "\n",
      "\n",
      "[Epoch 3]\n",
      "loss: 2.918933 [0/28137]\n",
      "loss: 2.925900 [1280/28137]\n",
      "loss: 2.906240 [2560/28137]\n",
      "loss: 2.903650 [3840/28137]\n",
      "loss: 2.851854 [5120/28137]\n",
      "loss: 2.841336 [6400/28137]\n",
      "loss: 2.866419 [7680/28137]\n",
      "loss: 2.905465 [8960/28137]\n",
      "loss: 2.887569 [10240/28137]\n",
      "loss: 2.885615 [11520/28137]\n",
      "loss: 2.920265 [12800/28137]\n",
      "loss: 2.920806 [14080/28137]\n",
      "loss: 2.890731 [15360/28137]\n",
      "loss: 2.867974 [16640/28137]\n",
      "loss: 2.840518 [17920/28137]\n",
      "loss: 2.864312 [19200/28137]\n",
      "loss: 2.904115 [20480/28137]\n",
      "loss: 2.972955 [21760/28137]\n",
      "loss: 2.906641 [23040/28137]\n",
      "loss: 2.856422 [24320/28137]\n",
      "loss: 2.918299 [25600/28137]\n",
      "loss: 2.902129 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.885990, Accuracy: 2774/3725 (74.5%)\n",
      "\n",
      "\n",
      "[Epoch 4]\n",
      "loss: 2.910731 [0/28137]\n",
      "loss: 2.934344 [1280/28137]\n",
      "loss: 2.923691 [2560/28137]\n",
      "loss: 2.921613 [3840/28137]\n",
      "loss: 2.913186 [5120/28137]\n",
      "loss: 2.903129 [6400/28137]\n",
      "loss: 2.900094 [7680/28137]\n",
      "loss: 2.898353 [8960/28137]\n",
      "loss: 2.841703 [10240/28137]\n",
      "loss: 2.840561 [11520/28137]\n",
      "loss: 2.832627 [12800/28137]\n",
      "loss: 2.829356 [14080/28137]\n",
      "loss: 2.841078 [15360/28137]\n",
      "loss: 2.895583 [16640/28137]\n",
      "loss: 2.881827 [17920/28137]\n",
      "loss: 2.926757 [19200/28137]\n",
      "loss: 2.965379 [20480/28137]\n",
      "loss: 2.826209 [21760/28137]\n",
      "loss: 2.912858 [23040/28137]\n",
      "loss: 2.887914 [24320/28137]\n",
      "loss: 2.851634 [25600/28137]\n",
      "loss: 2.860710 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.885688, Accuracy: 2773/3725 (74.4%)\n",
      "\n",
      "\n",
      "[Epoch 5]\n",
      "loss: 2.903263 [0/28137]\n",
      "loss: 2.910496 [1280/28137]\n",
      "loss: 2.879529 [2560/28137]\n",
      "loss: 2.864278 [3840/28137]\n",
      "loss: 2.926231 [5120/28137]\n",
      "loss: 2.897676 [6400/28137]\n",
      "loss: 2.886418 [7680/28137]\n",
      "loss: 2.802089 [8960/28137]\n",
      "loss: 2.863576 [10240/28137]\n",
      "loss: 2.915641 [11520/28137]\n",
      "loss: 2.973722 [12800/28137]\n",
      "loss: 2.895727 [14080/28137]\n",
      "loss: 2.935161 [15360/28137]\n",
      "loss: 2.926506 [16640/28137]\n",
      "loss: 2.889287 [17920/28137]\n",
      "loss: 2.909884 [19200/28137]\n",
      "loss: 2.848077 [20480/28137]\n",
      "loss: 2.887200 [21760/28137]\n",
      "loss: 2.873331 [23040/28137]\n",
      "loss: 2.909897 [24320/28137]\n",
      "loss: 2.966148 [25600/28137]\n",
      "loss: 2.925282 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.882603, Accuracy: 2783/3725 (74.7%)\n",
      "\n",
      "\n",
      "[Epoch 6]\n",
      "loss: 2.917973 [0/28137]\n",
      "loss: 2.949015 [1280/28137]\n",
      "loss: 2.902242 [2560/28137]\n",
      "loss: 2.879093 [3840/28137]\n",
      "loss: 2.925379 [5120/28137]\n",
      "loss: 2.840141 [6400/28137]\n",
      "loss: 2.942676 [7680/28137]\n",
      "loss: 2.925804 [8960/28137]\n",
      "loss: 2.902827 [10240/28137]\n",
      "loss: 2.917722 [11520/28137]\n",
      "loss: 2.840597 [12800/28137]\n",
      "loss: 2.910321 [14080/28137]\n",
      "loss: 2.835901 [15360/28137]\n",
      "loss: 2.942151 [16640/28137]\n",
      "loss: 2.956584 [17920/28137]\n",
      "loss: 2.926354 [19200/28137]\n",
      "loss: 2.886677 [20480/28137]\n",
      "loss: 2.933168 [21760/28137]\n",
      "loss: 2.870931 [23040/28137]\n",
      "loss: 2.963574 [24320/28137]\n",
      "loss: 2.878166 [25600/28137]\n",
      "loss: 2.871659 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.881050, Accuracy: 2786/3725 (74.8%)\n",
      "\n",
      "\n",
      "[Epoch 7]\n",
      "loss: 2.888282 [0/28137]\n",
      "loss: 2.933069 [1280/28137]\n",
      "loss: 2.878984 [2560/28137]\n",
      "loss: 2.910498 [3840/28137]\n",
      "loss: 2.871355 [5120/28137]\n",
      "loss: 2.868392 [6400/28137]\n",
      "loss: 2.933097 [7680/28137]\n",
      "loss: 2.940969 [8960/28137]\n",
      "loss: 2.889135 [10240/28137]\n",
      "loss: 2.909287 [11520/28137]\n",
      "loss: 2.901884 [12800/28137]\n",
      "loss: 2.870988 [14080/28137]\n",
      "loss: 2.917333 [15360/28137]\n",
      "loss: 2.909236 [16640/28137]\n",
      "loss: 2.832430 [17920/28137]\n",
      "loss: 2.840275 [19200/28137]\n",
      "loss: 2.894107 [20480/28137]\n",
      "loss: 2.788741 [21760/28137]\n",
      "loss: 2.909310 [23040/28137]\n",
      "loss: 2.857412 [24320/28137]\n",
      "loss: 2.920368 [25600/28137]\n",
      "loss: 2.812480 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.862208, Accuracy: 2861/3725 (76.8%)\n",
      "\n",
      "\n",
      "[Epoch 8]\n",
      "loss: 2.943911 [0/28137]\n",
      "loss: 2.850540 [1280/28137]\n",
      "loss: 2.887289 [2560/28137]\n",
      "loss: 2.930030 [3840/28137]\n",
      "loss: 2.827891 [5120/28137]\n",
      "loss: 2.804582 [6400/28137]\n",
      "loss: 2.874859 [7680/28137]\n",
      "loss: 2.894758 [8960/28137]\n",
      "loss: 2.870599 [10240/28137]\n",
      "loss: 2.816936 [11520/28137]\n",
      "loss: 2.886629 [12800/28137]\n",
      "loss: 2.810395 [14080/28137]\n",
      "loss: 2.832504 [15360/28137]\n",
      "loss: 2.801174 [16640/28137]\n",
      "loss: 2.833846 [17920/28137]\n",
      "loss: 2.847323 [19200/28137]\n",
      "loss: 2.855890 [20480/28137]\n",
      "loss: 2.825355 [21760/28137]\n",
      "loss: 2.863235 [23040/28137]\n",
      "loss: 2.855048 [24320/28137]\n",
      "loss: 2.801285 [25600/28137]\n",
      "loss: 2.886454 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.853192, Accuracy: 2890/3725 (77.6%)\n",
      "\n",
      "\n",
      "[Epoch 9]\n",
      "loss: 2.894439 [0/28137]\n",
      "loss: 2.894166 [1280/28137]\n",
      "loss: 2.839173 [2560/28137]\n",
      "loss: 2.825093 [3840/28137]\n",
      "loss: 2.863210 [5120/28137]\n",
      "loss: 2.848202 [6400/28137]\n",
      "loss: 2.876957 [7680/28137]\n",
      "loss: 2.878524 [8960/28137]\n",
      "loss: 2.840442 [10240/28137]\n",
      "loss: 2.870951 [11520/28137]\n",
      "loss: 2.863044 [12800/28137]\n",
      "loss: 2.841085 [14080/28137]\n",
      "loss: 2.848462 [15360/28137]\n",
      "loss: 2.865870 [16640/28137]\n",
      "loss: 2.886981 [17920/28137]\n",
      "loss: 2.841031 [19200/28137]\n",
      "loss: 2.894888 [20480/28137]\n",
      "loss: 2.855051 [21760/28137]\n",
      "loss: 2.816472 [23040/28137]\n",
      "loss: 2.847305 [24320/28137]\n",
      "loss: 2.878865 [25600/28137]\n",
      "loss: 2.909364 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.852795, Accuracy: 2892/3725 (77.6%)\n",
      "\n",
      "\n",
      "[Epoch 10]\n",
      "loss: 2.808160 [0/28137]\n",
      "loss: 2.870300 [1280/28137]\n",
      "loss: 2.831656 [2560/28137]\n",
      "loss: 2.855295 [3840/28137]\n",
      "loss: 2.777878 [5120/28137]\n",
      "loss: 2.831498 [6400/28137]\n",
      "loss: 2.855367 [7680/28137]\n",
      "loss: 2.824736 [8960/28137]\n",
      "loss: 2.785422 [10240/28137]\n",
      "loss: 2.870434 [11520/28137]\n",
      "loss: 2.893442 [12800/28137]\n",
      "loss: 2.854783 [14080/28137]\n",
      "loss: 2.884487 [15360/28137]\n",
      "loss: 2.832137 [16640/28137]\n",
      "loss: 2.909184 [17920/28137]\n",
      "loss: 2.762202 [19200/28137]\n",
      "loss: 2.856631 [20480/28137]\n",
      "loss: 2.878551 [21760/28137]\n",
      "loss: 2.866585 [23040/28137]\n",
      "loss: 2.901512 [24320/28137]\n",
      "loss: 2.855469 [25600/28137]\n",
      "loss: 2.870236 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.852357, Accuracy: 2892/3725 (77.6%)\n",
      "\n",
      "\n",
      "[Epoch 11]\n",
      "loss: 2.839885 [0/28137]\n",
      "loss: 2.855560 [1280/28137]\n",
      "loss: 2.832212 [2560/28137]\n",
      "loss: 2.862247 [3840/28137]\n",
      "loss: 2.900813 [5120/28137]\n",
      "loss: 2.870183 [6400/28137]\n",
      "loss: 2.761697 [7680/28137]\n",
      "loss: 2.847032 [8960/28137]\n",
      "loss: 2.823972 [10240/28137]\n",
      "loss: 2.823850 [11520/28137]\n",
      "loss: 2.785021 [12800/28137]\n",
      "loss: 2.839068 [14080/28137]\n",
      "loss: 2.893216 [15360/28137]\n",
      "loss: 2.831524 [16640/28137]\n",
      "loss: 2.823591 [17920/28137]\n",
      "loss: 2.932099 [19200/28137]\n",
      "loss: 2.878137 [20480/28137]\n",
      "loss: 2.808768 [21760/28137]\n",
      "loss: 2.923974 [23040/28137]\n",
      "loss: 2.823701 [24320/28137]\n",
      "loss: 2.823700 [25600/28137]\n",
      "loss: 2.878017 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.851644, Accuracy: 2893/3725 (77.7%)\n",
      "\n",
      "\n",
      "[Epoch 12]\n",
      "loss: 2.854610 [0/28137]\n",
      "loss: 2.862275 [1280/28137]\n",
      "loss: 2.846705 [2560/28137]\n",
      "loss: 2.862101 [3840/28137]\n",
      "loss: 2.831016 [5120/28137]\n",
      "loss: 2.839122 [6400/28137]\n",
      "loss: 2.823407 [7680/28137]\n",
      "loss: 2.838919 [8960/28137]\n",
      "loss: 2.846707 [10240/28137]\n",
      "loss: 2.931732 [11520/28137]\n",
      "loss: 2.815766 [12800/28137]\n",
      "loss: 2.816374 [14080/28137]\n",
      "loss: 2.870178 [15360/28137]\n",
      "loss: 2.923794 [16640/28137]\n",
      "loss: 2.854446 [17920/28137]\n",
      "loss: 2.839129 [19200/28137]\n",
      "loss: 2.838856 [20480/28137]\n",
      "loss: 2.823626 [21760/28137]\n",
      "loss: 2.916176 [23040/28137]\n",
      "loss: 2.885506 [24320/28137]\n",
      "loss: 2.916656 [25600/28137]\n",
      "loss: 2.862269 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.851812, Accuracy: 2891/3725 (77.6%)\n",
      "\n",
      "\n",
      "[Epoch 13]\n",
      "loss: 2.885527 [0/28137]\n",
      "loss: 2.862609 [1280/28137]\n",
      "loss: 2.838660 [2560/28137]\n",
      "loss: 2.831140 [3840/28137]\n",
      "loss: 2.808044 [5120/28137]\n",
      "loss: 2.808041 [6400/28137]\n",
      "loss: 2.808069 [7680/28137]\n",
      "loss: 2.908564 [8960/28137]\n",
      "loss: 2.838865 [10240/28137]\n",
      "loss: 2.870124 [11520/28137]\n",
      "loss: 2.900533 [12800/28137]\n",
      "loss: 2.870165 [14080/28137]\n",
      "loss: 2.862365 [15360/28137]\n",
      "loss: 2.846904 [16640/28137]\n",
      "loss: 2.815714 [17920/28137]\n",
      "loss: 2.846687 [19200/28137]\n",
      "loss: 2.838974 [20480/28137]\n",
      "loss: 2.869613 [21760/28137]\n",
      "loss: 2.885442 [23040/28137]\n",
      "loss: 2.854192 [24320/28137]\n",
      "loss: 2.935906 [25600/28137]\n",
      "loss: 2.792022 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.828871, Accuracy: 2985/3725 (80.1%)\n",
      "\n",
      "\n",
      "[Epoch 14]\n",
      "loss: 2.810504 [0/28137]\n",
      "loss: 2.824586 [1280/28137]\n",
      "loss: 2.856397 [2560/28137]\n",
      "loss: 2.847773 [3840/28137]\n",
      "loss: 2.847944 [5120/28137]\n",
      "loss: 2.874613 [6400/28137]\n",
      "loss: 2.902006 [7680/28137]\n",
      "loss: 2.824663 [8960/28137]\n",
      "loss: 2.816468 [10240/28137]\n",
      "loss: 2.792964 [11520/28137]\n",
      "loss: 2.809116 [12800/28137]\n",
      "loss: 2.808308 [14080/28137]\n",
      "loss: 2.823663 [15360/28137]\n",
      "loss: 2.815983 [16640/28137]\n",
      "loss: 2.809602 [17920/28137]\n",
      "loss: 2.801748 [19200/28137]\n",
      "loss: 2.805371 [20480/28137]\n",
      "loss: 2.862641 [21760/28137]\n",
      "loss: 2.808621 [23040/28137]\n",
      "loss: 2.825056 [24320/28137]\n",
      "loss: 2.824761 [25600/28137]\n",
      "loss: 2.847517 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.825577, Accuracy: 2993/3725 (80.3%)\n",
      "\n",
      "\n",
      "[Epoch 15]\n",
      "loss: 2.800744 [0/28137]\n",
      "loss: 2.754690 [1280/28137]\n",
      "loss: 2.882188 [2560/28137]\n",
      "loss: 2.893472 [3840/28137]\n",
      "loss: 2.862560 [5120/28137]\n",
      "loss: 2.878430 [6400/28137]\n",
      "loss: 2.769567 [7680/28137]\n",
      "loss: 2.805091 [8960/28137]\n",
      "loss: 2.814926 [10240/28137]\n",
      "loss: 2.769791 [11520/28137]\n",
      "loss: 2.808758 [12800/28137]\n",
      "loss: 2.863621 [14080/28137]\n",
      "loss: 2.849504 [15360/28137]\n",
      "loss: 2.823860 [16640/28137]\n",
      "loss: 2.839708 [17920/28137]\n",
      "loss: 2.824702 [19200/28137]\n",
      "loss: 2.854645 [20480/28137]\n",
      "loss: 2.809359 [21760/28137]\n",
      "loss: 2.828462 [23040/28137]\n",
      "loss: 2.832732 [24320/28137]\n",
      "loss: 2.909587 [25600/28137]\n",
      "loss: 2.847378 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.825135, Accuracy: 2994/3725 (80.4%)\n",
      "\n",
      "\n",
      "[Epoch 16]\n",
      "loss: 2.831929 [0/28137]\n",
      "loss: 2.823846 [1280/28137]\n",
      "loss: 2.854540 [2560/28137]\n",
      "loss: 2.800515 [3840/28137]\n",
      "loss: 2.823404 [5120/28137]\n",
      "loss: 2.846898 [6400/28137]\n",
      "loss: 2.823438 [7680/28137]\n",
      "loss: 2.823735 [8960/28137]\n",
      "loss: 2.800394 [10240/28137]\n",
      "loss: 2.823648 [11520/28137]\n",
      "loss: 2.839061 [12800/28137]\n",
      "loss: 2.838112 [14080/28137]\n",
      "loss: 2.802157 [15360/28137]\n",
      "loss: 2.838993 [16640/28137]\n",
      "loss: 2.878525 [17920/28137]\n",
      "loss: 2.786658 [19200/28137]\n",
      "loss: 2.777840 [20480/28137]\n",
      "loss: 2.850275 [21760/28137]\n",
      "loss: 2.842131 [23040/28137]\n",
      "loss: 2.755892 [24320/28137]\n",
      "loss: 2.809606 [25600/28137]\n",
      "loss: 2.804097 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.801290, Accuracy: 3089/3725 (82.9%)\n",
      "\n",
      "\n",
      "[Epoch 17]\n",
      "loss: 2.809732 [0/28137]\n",
      "loss: 2.772032 [1280/28137]\n",
      "loss: 2.841399 [2560/28137]\n",
      "loss: 2.793492 [3840/28137]\n",
      "loss: 2.765307 [5120/28137]\n",
      "loss: 2.749500 [6400/28137]\n",
      "loss: 2.739377 [7680/28137]\n",
      "loss: 2.777929 [8960/28137]\n",
      "loss: 2.769795 [10240/28137]\n",
      "loss: 2.763461 [11520/28137]\n",
      "loss: 2.731721 [12800/28137]\n",
      "loss: 2.821176 [14080/28137]\n",
      "loss: 2.703595 [15360/28137]\n",
      "loss: 2.763491 [16640/28137]\n",
      "loss: 2.779915 [17920/28137]\n",
      "loss: 2.739236 [19200/28137]\n",
      "loss: 2.782633 [20480/28137]\n",
      "loss: 2.755674 [21760/28137]\n",
      "loss: 2.743278 [23040/28137]\n",
      "loss: 2.778916 [24320/28137]\n",
      "loss: 2.754374 [25600/28137]\n",
      "loss: 2.738882 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.739986, Accuracy: 3316/3725 (89.0%)\n",
      "\n",
      "\n",
      "[Epoch 18]\n",
      "loss: 2.731722 [0/28137]\n",
      "loss: 2.723616 [1280/28137]\n",
      "loss: 2.724028 [2560/28137]\n",
      "loss: 2.774490 [3840/28137]\n",
      "loss: 2.715506 [5120/28137]\n",
      "loss: 2.692165 [6400/28137]\n",
      "loss: 2.739072 [7680/28137]\n",
      "loss: 2.739045 [8960/28137]\n",
      "loss: 2.771008 [10240/28137]\n",
      "loss: 2.731870 [11520/28137]\n",
      "loss: 2.754108 [12800/28137]\n",
      "loss: 2.731249 [14080/28137]\n",
      "loss: 2.754283 [15360/28137]\n",
      "loss: 2.746205 [16640/28137]\n",
      "loss: 2.761973 [17920/28137]\n",
      "loss: 2.723054 [19200/28137]\n",
      "loss: 2.753821 [20480/28137]\n",
      "loss: 2.723205 [21760/28137]\n",
      "loss: 2.715443 [23040/28137]\n",
      "loss: 2.761720 [24320/28137]\n",
      "loss: 2.738673 [25600/28137]\n",
      "loss: 2.699941 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.739120, Accuracy: 3316/3725 (89.0%)\n",
      "\n",
      "\n",
      "[Epoch 19]\n",
      "loss: 2.699897 [0/28137]\n",
      "loss: 2.746516 [1280/28137]\n",
      "loss: 2.707917 [2560/28137]\n",
      "loss: 2.738663 [3840/28137]\n",
      "loss: 2.699799 [5120/28137]\n",
      "loss: 2.754091 [6400/28137]\n",
      "loss: 2.753894 [7680/28137]\n",
      "loss: 2.707450 [8960/28137]\n",
      "loss: 2.723100 [10240/28137]\n",
      "loss: 2.777250 [11520/28137]\n",
      "loss: 2.730694 [12800/28137]\n",
      "loss: 2.746436 [14080/28137]\n",
      "loss: 2.715132 [15360/28137]\n",
      "loss: 2.676621 [16640/28137]\n",
      "loss: 2.753738 [17920/28137]\n",
      "loss: 2.769218 [19200/28137]\n",
      "loss: 2.661121 [20480/28137]\n",
      "loss: 2.753915 [21760/28137]\n",
      "loss: 2.738505 [23040/28137]\n",
      "loss: 2.769136 [24320/28137]\n",
      "loss: 2.769131 [25600/28137]\n",
      "loss: 2.761481 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.737653, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 20]\n",
      "loss: 2.753677 [0/28137]\n",
      "loss: 2.769476 [1280/28137]\n",
      "loss: 2.746208 [2560/28137]\n",
      "loss: 2.722879 [3840/28137]\n",
      "loss: 2.784571 [5120/28137]\n",
      "loss: 2.699627 [6400/28137]\n",
      "loss: 2.746119 [7680/28137]\n",
      "loss: 2.738335 [8960/28137]\n",
      "loss: 2.730696 [10240/28137]\n",
      "loss: 2.723002 [11520/28137]\n",
      "loss: 2.769372 [12800/28137]\n",
      "loss: 2.715045 [14080/28137]\n",
      "loss: 2.753725 [15360/28137]\n",
      "loss: 2.753798 [16640/28137]\n",
      "loss: 2.769131 [17920/28137]\n",
      "loss: 2.761457 [19200/28137]\n",
      "loss: 2.792346 [20480/28137]\n",
      "loss: 2.722797 [21760/28137]\n",
      "loss: 2.730522 [23040/28137]\n",
      "loss: 2.738517 [24320/28137]\n",
      "loss: 2.722742 [25600/28137]\n",
      "loss: 2.715117 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.737487, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 21]\n",
      "loss: 2.745820 [0/28137]\n",
      "loss: 2.730385 [1280/28137]\n",
      "loss: 2.730523 [2560/28137]\n",
      "loss: 2.707330 [3840/28137]\n",
      "loss: 2.745824 [5120/28137]\n",
      "loss: 2.753551 [6400/28137]\n",
      "loss: 2.668739 [7680/28137]\n",
      "loss: 2.730528 [8960/28137]\n",
      "loss: 2.722893 [10240/28137]\n",
      "loss: 2.738291 [11520/28137]\n",
      "loss: 2.715291 [12800/28137]\n",
      "loss: 2.745924 [14080/28137]\n",
      "loss: 2.753707 [15360/28137]\n",
      "loss: 2.707213 [16640/28137]\n",
      "loss: 2.691955 [17920/28137]\n",
      "loss: 2.730541 [19200/28137]\n",
      "loss: 2.784531 [20480/28137]\n",
      "loss: 2.776773 [21760/28137]\n",
      "loss: 2.722935 [23040/28137]\n",
      "loss: 2.738172 [24320/28137]\n",
      "loss: 2.776809 [25600/28137]\n",
      "loss: 2.714959 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.737371, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 22]\n",
      "loss: 2.768919 [0/28137]\n",
      "loss: 2.823014 [1280/28137]\n",
      "loss: 2.730482 [2560/28137]\n",
      "loss: 2.722800 [3840/28137]\n",
      "loss: 2.745777 [5120/28137]\n",
      "loss: 2.715117 [6400/28137]\n",
      "loss: 2.738099 [7680/28137]\n",
      "loss: 2.738093 [8960/28137]\n",
      "loss: 2.722714 [10240/28137]\n",
      "loss: 2.714955 [11520/28137]\n",
      "loss: 2.738154 [12800/28137]\n",
      "loss: 2.722666 [14080/28137]\n",
      "loss: 2.722791 [15360/28137]\n",
      "loss: 2.730357 [16640/28137]\n",
      "loss: 2.745785 [17920/28137]\n",
      "loss: 2.730474 [19200/28137]\n",
      "loss: 2.776694 [20480/28137]\n",
      "loss: 2.753934 [21760/28137]\n",
      "loss: 2.715178 [23040/28137]\n",
      "loss: 2.715131 [24320/28137]\n",
      "loss: 2.792001 [25600/28137]\n",
      "loss: 2.753546 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.737347, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 23]\n",
      "loss: 2.753493 [0/28137]\n",
      "loss: 2.792009 [1280/28137]\n",
      "loss: 2.768885 [2560/28137]\n",
      "loss: 2.707456 [3840/28137]\n",
      "loss: 2.738228 [5120/28137]\n",
      "loss: 2.738222 [6400/28137]\n",
      "loss: 2.722678 [7680/28137]\n",
      "loss: 2.714951 [8960/28137]\n",
      "loss: 2.715016 [10240/28137]\n",
      "loss: 2.776746 [11520/28137]\n",
      "loss: 2.722663 [12800/28137]\n",
      "loss: 2.699558 [14080/28137]\n",
      "loss: 2.730430 [15360/28137]\n",
      "loss: 2.730423 [16640/28137]\n",
      "loss: 2.761097 [17920/28137]\n",
      "loss: 2.730272 [19200/28137]\n",
      "loss: 2.753614 [20480/28137]\n",
      "loss: 2.715072 [21760/28137]\n",
      "loss: 2.761201 [23040/28137]\n",
      "loss: 2.761227 [24320/28137]\n",
      "loss: 2.707293 [25600/28137]\n",
      "loss: 2.753652 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.737255, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 24]\n",
      "loss: 2.707221 [0/28137]\n",
      "loss: 2.730310 [1280/28137]\n",
      "loss: 2.699548 [2560/28137]\n",
      "loss: 2.714984 [3840/28137]\n",
      "loss: 2.768692 [5120/28137]\n",
      "loss: 2.730416 [6400/28137]\n",
      "loss: 2.684041 [7680/28137]\n",
      "loss: 2.745857 [8960/28137]\n",
      "loss: 2.715016 [10240/28137]\n",
      "loss: 2.745846 [11520/28137]\n",
      "loss: 2.730369 [12800/28137]\n",
      "loss: 2.738024 [14080/28137]\n",
      "loss: 2.722672 [15360/28137]\n",
      "loss: 2.753424 [16640/28137]\n",
      "loss: 2.738055 [17920/28137]\n",
      "loss: 2.745667 [19200/28137]\n",
      "loss: 2.737935 [20480/28137]\n",
      "loss: 2.753455 [21760/28137]\n",
      "loss: 2.761120 [23040/28137]\n",
      "loss: 2.722784 [24320/28137]\n",
      "loss: 2.776638 [25600/28137]\n",
      "loss: 2.776614 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.737256, Accuracy: 3321/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 25]\n",
      "loss: 2.745634 [0/28137]\n",
      "loss: 2.799723 [1280/28137]\n",
      "loss: 2.753328 [2560/28137]\n",
      "loss: 2.737925 [3840/28137]\n",
      "loss: 2.730332 [5120/28137]\n",
      "loss: 2.784370 [6400/28137]\n",
      "loss: 2.722607 [7680/28137]\n",
      "loss: 2.737967 [8960/28137]\n",
      "loss: 2.714833 [10240/28137]\n",
      "loss: 2.753428 [11520/28137]\n",
      "loss: 2.737935 [12800/28137]\n",
      "loss: 2.738032 [14080/28137]\n",
      "loss: 2.784256 [15360/28137]\n",
      "loss: 2.753376 [16640/28137]\n",
      "loss: 2.707231 [17920/28137]\n",
      "loss: 2.776579 [19200/28137]\n",
      "loss: 2.807382 [20480/28137]\n",
      "loss: 2.707232 [21760/28137]\n",
      "loss: 2.714891 [23040/28137]\n",
      "loss: 2.737974 [24320/28137]\n",
      "loss: 2.714918 [25600/28137]\n",
      "loss: 2.730385 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.737115, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 26]\n",
      "loss: 2.753399 [0/28137]\n",
      "loss: 2.714898 [1280/28137]\n",
      "loss: 2.738051 [2560/28137]\n",
      "loss: 2.776579 [3840/28137]\n",
      "loss: 2.699635 [5120/28137]\n",
      "loss: 2.737974 [6400/28137]\n",
      "loss: 2.737971 [7680/28137]\n",
      "loss: 2.699471 [8960/28137]\n",
      "loss: 2.745782 [10240/28137]\n",
      "loss: 2.722577 [11520/28137]\n",
      "loss: 2.730212 [12800/28137]\n",
      "loss: 2.714818 [14080/28137]\n",
      "loss: 2.714813 [15360/28137]\n",
      "loss: 2.714825 [16640/28137]\n",
      "loss: 2.784131 [17920/28137]\n",
      "loss: 2.753361 [19200/28137]\n",
      "loss: 2.738186 [20480/28137]\n",
      "loss: 2.753382 [21760/28137]\n",
      "loss: 2.768706 [23040/28137]\n",
      "loss: 2.776462 [24320/28137]\n",
      "loss: 2.730249 [25600/28137]\n",
      "loss: 2.753573 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.737093, Accuracy: 3321/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 27]\n",
      "loss: 2.761333 [0/28137]\n",
      "loss: 2.730231 [1280/28137]\n",
      "loss: 2.784168 [2560/28137]\n",
      "loss: 2.737883 [3840/28137]\n",
      "loss: 2.760971 [5120/28137]\n",
      "loss: 2.730509 [6400/28137]\n",
      "loss: 2.769022 [7680/28137]\n",
      "loss: 2.753394 [8960/28137]\n",
      "loss: 2.814836 [10240/28137]\n",
      "loss: 2.791819 [11520/28137]\n",
      "loss: 2.737954 [12800/28137]\n",
      "loss: 2.745626 [14080/28137]\n",
      "loss: 2.776447 [15360/28137]\n",
      "loss: 2.737935 [16640/28137]\n",
      "loss: 2.768644 [17920/28137]\n",
      "loss: 2.753369 [19200/28137]\n",
      "loss: 2.722553 [20480/28137]\n",
      "loss: 2.714874 [21760/28137]\n",
      "loss: 2.761109 [23040/28137]\n",
      "loss: 2.753324 [24320/28137]\n",
      "loss: 2.760991 [25600/28137]\n",
      "loss: 2.714845 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.737039, Accuracy: 3321/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 28]\n",
      "loss: 2.738091 [0/28137]\n",
      "loss: 2.722580 [1280/28137]\n",
      "loss: 2.707134 [2560/28137]\n",
      "loss: 2.714811 [3840/28137]\n",
      "loss: 2.760995 [5120/28137]\n",
      "loss: 2.753451 [6400/28137]\n",
      "loss: 2.761013 [7680/28137]\n",
      "loss: 2.761035 [8960/28137]\n",
      "loss: 2.753310 [10240/28137]\n",
      "loss: 2.745568 [11520/28137]\n",
      "loss: 2.730238 [12800/28137]\n",
      "loss: 2.761070 [14080/28137]\n",
      "loss: 2.722686 [15360/28137]\n",
      "loss: 2.761035 [16640/28137]\n",
      "loss: 2.722522 [17920/28137]\n",
      "loss: 2.722494 [19200/28137]\n",
      "loss: 2.753326 [20480/28137]\n",
      "loss: 2.737939 [21760/28137]\n",
      "loss: 2.668594 [23040/28137]\n",
      "loss: 2.753232 [24320/28137]\n",
      "loss: 2.722461 [25600/28137]\n",
      "loss: 2.745671 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.736967, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 29]\n",
      "loss: 2.691826 [0/28137]\n",
      "loss: 2.753308 [1280/28137]\n",
      "loss: 2.784161 [2560/28137]\n",
      "loss: 2.745717 [3840/28137]\n",
      "loss: 2.707104 [5120/28137]\n",
      "loss: 2.722485 [6400/28137]\n",
      "loss: 2.707064 [7680/28137]\n",
      "loss: 2.730294 [8960/28137]\n",
      "loss: 2.761003 [10240/28137]\n",
      "loss: 2.753260 [11520/28137]\n",
      "loss: 2.722491 [12800/28137]\n",
      "loss: 2.753322 [14080/28137]\n",
      "loss: 2.745653 [15360/28137]\n",
      "loss: 2.768770 [16640/28137]\n",
      "loss: 2.737824 [17920/28137]\n",
      "loss: 2.737818 [19200/28137]\n",
      "loss: 2.753242 [20480/28137]\n",
      "loss: 2.745735 [21760/28137]\n",
      "loss: 2.768704 [23040/28137]\n",
      "loss: 2.714879 [24320/28137]\n",
      "loss: 2.784171 [25600/28137]\n",
      "loss: 2.745687 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.736952, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 30]\n",
      "loss: 2.730178 [0/28137]\n",
      "loss: 2.799411 [1280/28137]\n",
      "loss: 2.707138 [2560/28137]\n",
      "loss: 2.737967 [3840/28137]\n",
      "loss: 2.745605 [5120/28137]\n",
      "loss: 2.691872 [6400/28137]\n",
      "loss: 2.737874 [7680/28137]\n",
      "loss: 2.737854 [8960/28137]\n",
      "loss: 2.760934 [10240/28137]\n",
      "loss: 2.699379 [11520/28137]\n",
      "loss: 2.776673 [12800/28137]\n",
      "loss: 2.722493 [14080/28137]\n",
      "loss: 2.707063 [15360/28137]\n",
      "loss: 2.730180 [16640/28137]\n",
      "loss: 2.753247 [17920/28137]\n",
      "loss: 2.722464 [19200/28137]\n",
      "loss: 2.745849 [20480/28137]\n",
      "loss: 2.761026 [21760/28137]\n",
      "loss: 2.745526 [23040/28137]\n",
      "loss: 2.714842 [24320/28137]\n",
      "loss: 2.730119 [25600/28137]\n",
      "loss: 2.753218 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.736971, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 31]\n",
      "loss: 2.737936 [0/28137]\n",
      "loss: 2.730185 [1280/28137]\n",
      "loss: 2.722533 [2560/28137]\n",
      "loss: 2.684007 [3840/28137]\n",
      "loss: 2.745639 [5120/28137]\n",
      "loss: 2.737848 [6400/28137]\n",
      "loss: 2.714723 [7680/28137]\n",
      "loss: 2.737819 [8960/28137]\n",
      "loss: 2.730158 [10240/28137]\n",
      "loss: 2.722451 [11520/28137]\n",
      "loss: 2.753273 [12800/28137]\n",
      "loss: 2.768623 [14080/28137]\n",
      "loss: 2.768651 [15360/28137]\n",
      "loss: 2.722558 [16640/28137]\n",
      "loss: 2.745561 [17920/28137]\n",
      "loss: 2.714823 [19200/28137]\n",
      "loss: 2.760930 [20480/28137]\n",
      "loss: 2.707041 [21760/28137]\n",
      "loss: 2.730148 [23040/28137]\n",
      "loss: 2.737799 [24320/28137]\n",
      "loss: 2.722561 [25600/28137]\n",
      "loss: 2.714872 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.736937, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 32]\n",
      "loss: 2.737844 [0/28137]\n",
      "loss: 2.768756 [1280/28137]\n",
      "loss: 2.730157 [2560/28137]\n",
      "loss: 2.707039 [3840/28137]\n",
      "loss: 2.776272 [5120/28137]\n",
      "loss: 2.768625 [6400/28137]\n",
      "loss: 2.745572 [7680/28137]\n",
      "loss: 2.760982 [8960/28137]\n",
      "loss: 2.737929 [10240/28137]\n",
      "loss: 2.745581 [11520/28137]\n",
      "loss: 2.730172 [12800/28137]\n",
      "loss: 2.753294 [14080/28137]\n",
      "loss: 2.737778 [15360/28137]\n",
      "loss: 2.783978 [16640/28137]\n",
      "loss: 2.745643 [17920/28137]\n",
      "loss: 2.784118 [19200/28137]\n",
      "loss: 2.699419 [20480/28137]\n",
      "loss: 2.699410 [21760/28137]\n",
      "loss: 2.699386 [23040/28137]\n",
      "loss: 2.730083 [24320/28137]\n",
      "loss: 2.768781 [25600/28137]\n",
      "loss: 2.699393 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.736903, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 33]\n",
      "loss: 2.745471 [0/28137]\n",
      "loss: 2.784029 [1280/28137]\n",
      "loss: 2.707222 [2560/28137]\n",
      "loss: 2.730232 [3840/28137]\n",
      "loss: 2.768668 [5120/28137]\n",
      "loss: 2.683955 [6400/28137]\n",
      "loss: 2.738022 [7680/28137]\n",
      "loss: 2.730127 [8960/28137]\n",
      "loss: 2.784053 [10240/28137]\n",
      "loss: 2.737865 [11520/28137]\n",
      "loss: 2.776292 [12800/28137]\n",
      "loss: 2.722419 [14080/28137]\n",
      "loss: 2.783990 [15360/28137]\n",
      "loss: 2.714761 [16640/28137]\n",
      "loss: 2.714739 [17920/28137]\n",
      "loss: 2.660902 [19200/28137]\n",
      "loss: 2.784226 [20480/28137]\n",
      "loss: 2.722497 [21760/28137]\n",
      "loss: 2.776289 [23040/28137]\n",
      "loss: 2.730168 [24320/28137]\n",
      "loss: 2.722436 [25600/28137]\n",
      "loss: 2.722431 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.736828, Accuracy: 3322/3725 (89.2%)\n",
      "\n",
      "\n",
      "[Epoch 34]\n",
      "loss: 2.760939 [0/28137]\n",
      "loss: 2.776279 [1280/28137]\n",
      "loss: 2.791747 [2560/28137]\n",
      "loss: 2.791724 [3840/28137]\n",
      "loss: 2.722456 [5120/28137]\n",
      "loss: 2.753260 [6400/28137]\n",
      "loss: 2.730134 [7680/28137]\n",
      "loss: 2.722544 [8960/28137]\n",
      "loss: 2.722433 [10240/28137]\n",
      "loss: 2.699354 [11520/28137]\n",
      "loss: 2.753205 [12800/28137]\n",
      "loss: 2.699397 [14080/28137]\n",
      "loss: 2.760922 [15360/28137]\n",
      "loss: 2.730124 [16640/28137]\n",
      "loss: 2.737760 [17920/28137]\n",
      "loss: 2.760868 [19200/28137]\n",
      "loss: 2.699392 [20480/28137]\n",
      "loss: 2.722322 [21760/28137]\n",
      "loss: 2.716013 [23040/28137]\n",
      "loss: 2.752608 [24320/28137]\n",
      "loss: 2.694937 [25600/28137]\n",
      "loss: 2.757686 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.719370, Accuracy: 3398/3725 (91.2%)\n",
      "\n",
      "\n",
      "[Epoch 35]\n",
      "loss: 2.679687 [0/28137]\n",
      "loss: 2.700239 [1280/28137]\n",
      "loss: 2.762952 [2560/28137]\n",
      "loss: 2.717602 [3840/28137]\n",
      "loss: 2.734267 [5120/28137]\n",
      "loss: 2.738965 [6400/28137]\n",
      "loss: 2.708235 [7680/28137]\n",
      "loss: 2.684557 [8960/28137]\n",
      "loss: 2.700856 [10240/28137]\n",
      "loss: 2.754575 [11520/28137]\n",
      "loss: 2.700220 [12800/28137]\n",
      "loss: 2.684461 [14080/28137]\n",
      "loss: 2.708096 [15360/28137]\n",
      "loss: 2.717674 [16640/28137]\n",
      "loss: 2.708155 [17920/28137]\n",
      "loss: 2.684798 [19200/28137]\n",
      "loss: 2.669744 [20480/28137]\n",
      "loss: 2.705729 [21760/28137]\n",
      "loss: 2.715551 [23040/28137]\n",
      "loss: 2.723431 [24320/28137]\n",
      "loss: 2.708135 [25600/28137]\n",
      "loss: 2.707836 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.708707, Accuracy: 3431/3725 (92.1%)\n",
      "\n",
      "\n",
      "[Epoch 36]\n",
      "loss: 2.762073 [0/28137]\n",
      "loss: 2.718776 [1280/28137]\n",
      "loss: 2.707406 [2560/28137]\n",
      "loss: 2.722994 [3840/28137]\n",
      "loss: 2.684422 [5120/28137]\n",
      "loss: 2.738357 [6400/28137]\n",
      "loss: 2.730782 [7680/28137]\n",
      "loss: 2.699705 [8960/28137]\n",
      "loss: 2.715126 [10240/28137]\n",
      "loss: 2.733513 [11520/28137]\n",
      "loss: 2.676430 [12800/28137]\n",
      "loss: 2.716558 [14080/28137]\n",
      "loss: 2.691883 [15360/28137]\n",
      "loss: 2.692457 [16640/28137]\n",
      "loss: 2.715150 [17920/28137]\n",
      "loss: 2.699952 [19200/28137]\n",
      "loss: 2.723293 [20480/28137]\n",
      "loss: 2.700330 [21760/28137]\n",
      "loss: 2.671164 [23040/28137]\n",
      "loss: 2.680565 [24320/28137]\n",
      "loss: 2.677046 [25600/28137]\n",
      "loss: 2.669067 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.689448, Accuracy: 3504/3725 (94.1%)\n",
      "\n",
      "\n",
      "[Epoch 37]\n",
      "loss: 2.676989 [0/28137]\n",
      "loss: 2.668968 [1280/28137]\n",
      "loss: 2.707685 [2560/28137]\n",
      "loss: 2.684177 [3840/28137]\n",
      "loss: 2.715063 [5120/28137]\n",
      "loss: 2.722982 [6400/28137]\n",
      "loss: 2.691951 [7680/28137]\n",
      "loss: 2.707331 [8960/28137]\n",
      "loss: 2.707376 [10240/28137]\n",
      "loss: 2.676548 [11520/28137]\n",
      "loss: 2.707474 [12800/28137]\n",
      "loss: 2.691801 [14080/28137]\n",
      "loss: 2.676501 [15360/28137]\n",
      "loss: 2.691929 [16640/28137]\n",
      "loss: 2.684237 [17920/28137]\n",
      "loss: 2.668676 [19200/28137]\n",
      "loss: 2.676581 [20480/28137]\n",
      "loss: 2.676600 [21760/28137]\n",
      "loss: 2.676470 [23040/28137]\n",
      "loss: 2.661110 [24320/28137]\n",
      "loss: 2.707366 [25600/28137]\n",
      "loss: 2.707470 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.688050, Accuracy: 3506/3725 (94.1%)\n",
      "\n",
      "\n",
      "[Epoch 38]\n",
      "loss: 2.684071 [0/28137]\n",
      "loss: 2.684294 [1280/28137]\n",
      "loss: 2.684092 [2560/28137]\n",
      "loss: 2.691764 [3840/28137]\n",
      "loss: 2.699587 [5120/28137]\n",
      "loss: 2.684075 [6400/28137]\n",
      "loss: 2.691814 [7680/28137]\n",
      "loss: 2.676465 [8960/28137]\n",
      "loss: 2.722566 [10240/28137]\n",
      "loss: 2.691775 [11520/28137]\n",
      "loss: 2.707569 [12800/28137]\n",
      "loss: 2.653234 [14080/28137]\n",
      "loss: 2.684173 [15360/28137]\n",
      "loss: 2.707135 [16640/28137]\n",
      "loss: 2.714954 [17920/28137]\n",
      "loss: 2.684036 [19200/28137]\n",
      "loss: 2.684059 [20480/28137]\n",
      "loss: 2.660978 [21760/28137]\n",
      "loss: 2.653398 [23040/28137]\n",
      "loss: 2.691786 [24320/28137]\n",
      "loss: 2.684135 [25600/28137]\n",
      "loss: 2.661055 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.687860, Accuracy: 3507/3725 (94.1%)\n",
      "\n",
      "\n",
      "[Epoch 39]\n",
      "loss: 2.684001 [0/28137]\n",
      "loss: 2.684047 [1280/28137]\n",
      "loss: 2.676387 [2560/28137]\n",
      "loss: 2.676348 [3840/28137]\n",
      "loss: 2.676353 [5120/28137]\n",
      "loss: 2.668640 [6400/28137]\n",
      "loss: 2.684005 [7680/28137]\n",
      "loss: 2.707128 [8960/28137]\n",
      "loss: 2.684036 [10240/28137]\n",
      "loss: 2.699434 [11520/28137]\n",
      "loss: 2.653234 [12800/28137]\n",
      "loss: 2.699420 [14080/28137]\n",
      "loss: 2.653225 [15360/28137]\n",
      "loss: 2.676433 [16640/28137]\n",
      "loss: 2.714830 [17920/28137]\n",
      "loss: 2.691677 [19200/28137]\n",
      "loss: 2.676337 [20480/28137]\n",
      "loss: 2.699420 [21760/28137]\n",
      "loss: 2.730198 [23040/28137]\n",
      "loss: 2.691685 [24320/28137]\n",
      "loss: 2.684088 [25600/28137]\n",
      "loss: 2.714756 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.687806, Accuracy: 3507/3725 (94.1%)\n",
      "\n",
      "\n",
      "[Epoch 40]\n",
      "loss: 2.668769 [0/28137]\n",
      "loss: 2.691715 [1280/28137]\n",
      "loss: 2.707149 [2560/28137]\n",
      "loss: 2.668664 [3840/28137]\n",
      "loss: 2.699337 [5120/28137]\n",
      "loss: 2.699466 [6400/28137]\n",
      "loss: 2.676300 [7680/28137]\n",
      "loss: 2.653248 [8960/28137]\n",
      "loss: 2.684021 [10240/28137]\n",
      "loss: 2.676321 [11520/28137]\n",
      "loss: 2.699479 [12800/28137]\n",
      "loss: 2.630153 [14080/28137]\n",
      "loss: 2.691721 [15360/28137]\n",
      "loss: 2.699424 [16640/28137]\n",
      "loss: 2.699387 [17920/28137]\n",
      "loss: 2.730177 [19200/28137]\n",
      "loss: 2.683999 [20480/28137]\n",
      "loss: 2.691711 [21760/28137]\n",
      "loss: 2.714761 [23040/28137]\n",
      "loss: 2.707207 [24320/28137]\n",
      "loss: 2.684019 [25600/28137]\n",
      "loss: 2.675283 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.660202, Accuracy: 3613/3725 (97.0%)\n",
      "\n",
      "\n",
      "[Epoch 41]\n",
      "loss: 2.661105 [0/28137]\n",
      "loss: 2.661556 [1280/28137]\n",
      "loss: 2.653489 [2560/28137]\n",
      "loss: 2.661112 [3840/28137]\n",
      "loss: 2.668881 [5120/28137]\n",
      "loss: 2.699841 [6400/28137]\n",
      "loss: 2.645668 [7680/28137]\n",
      "loss: 2.648601 [8960/28137]\n",
      "loss: 2.653339 [10240/28137]\n",
      "loss: 2.661040 [11520/28137]\n",
      "loss: 2.653418 [12800/28137]\n",
      "loss: 2.661046 [14080/28137]\n",
      "loss: 2.645633 [15360/28137]\n",
      "loss: 2.668678 [16640/28137]\n",
      "loss: 2.676528 [17920/28137]\n",
      "loss: 2.662329 [19200/28137]\n",
      "loss: 2.653662 [20480/28137]\n",
      "loss: 2.668529 [21760/28137]\n",
      "loss: 2.662882 [23040/28137]\n",
      "loss: 2.677859 [24320/28137]\n",
      "loss: 2.676835 [25600/28137]\n",
      "loss: 2.687090 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.661391, Accuracy: 3610/3725 (96.9%)\n",
      "\n",
      "\n",
      "[Epoch 42]\n",
      "loss: 2.708297 [0/28137]\n",
      "loss: 2.653593 [1280/28137]\n",
      "loss: 2.659531 [2560/28137]\n",
      "loss: 2.630164 [3840/28137]\n",
      "loss: 2.668831 [5120/28137]\n",
      "loss: 2.661468 [6400/28137]\n",
      "loss: 2.645773 [7680/28137]\n",
      "loss: 2.655547 [8960/28137]\n",
      "loss: 2.693366 [10240/28137]\n",
      "loss: 2.668999 [11520/28137]\n",
      "loss: 2.698135 [12800/28137]\n",
      "loss: 2.661266 [14080/28137]\n",
      "loss: 2.661529 [15360/28137]\n",
      "loss: 2.684268 [16640/28137]\n",
      "loss: 2.658574 [17920/28137]\n",
      "loss: 2.653559 [19200/28137]\n",
      "loss: 2.653420 [20480/28137]\n",
      "loss: 2.637876 [21760/28137]\n",
      "loss: 2.653313 [23040/28137]\n",
      "loss: 2.630184 [24320/28137]\n",
      "loss: 2.653367 [25600/28137]\n",
      "loss: 2.637915 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658461, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 43]\n",
      "loss: 2.661043 [0/28137]\n",
      "loss: 2.653350 [1280/28137]\n",
      "loss: 2.653293 [2560/28137]\n",
      "loss: 2.653407 [3840/28137]\n",
      "loss: 2.668928 [5120/28137]\n",
      "loss: 2.668736 [6400/28137]\n",
      "loss: 2.676586 [7680/28137]\n",
      "loss: 2.660979 [8960/28137]\n",
      "loss: 2.691934 [10240/28137]\n",
      "loss: 2.653257 [11520/28137]\n",
      "loss: 2.653286 [12800/28137]\n",
      "loss: 2.660973 [14080/28137]\n",
      "loss: 2.630148 [15360/28137]\n",
      "loss: 2.653369 [16640/28137]\n",
      "loss: 2.660942 [17920/28137]\n",
      "loss: 2.676337 [19200/28137]\n",
      "loss: 2.699438 [20480/28137]\n",
      "loss: 2.660969 [21760/28137]\n",
      "loss: 2.668755 [23040/28137]\n",
      "loss: 2.637853 [24320/28137]\n",
      "loss: 2.653232 [25600/28137]\n",
      "loss: 2.660955 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658737, Accuracy: 3617/3725 (97.1%)\n",
      "\n",
      "\n",
      "[Epoch 44]\n",
      "loss: 2.653276 [0/28137]\n",
      "loss: 2.668689 [1280/28137]\n",
      "loss: 2.668782 [2560/28137]\n",
      "loss: 2.692040 [3840/28137]\n",
      "loss: 2.645586 [5120/28137]\n",
      "loss: 2.645555 [6400/28137]\n",
      "loss: 2.676450 [7680/28137]\n",
      "loss: 2.645864 [8960/28137]\n",
      "loss: 2.660938 [10240/28137]\n",
      "loss: 2.637855 [11520/28137]\n",
      "loss: 2.630146 [12800/28137]\n",
      "loss: 2.660939 [14080/28137]\n",
      "loss: 2.676366 [15360/28137]\n",
      "loss: 2.630148 [16640/28137]\n",
      "loss: 2.660959 [17920/28137]\n",
      "loss: 2.637855 [19200/28137]\n",
      "loss: 2.653262 [20480/28137]\n",
      "loss: 2.660952 [21760/28137]\n",
      "loss: 2.676314 [23040/28137]\n",
      "loss: 2.684032 [24320/28137]\n",
      "loss: 2.645537 [25600/28137]\n",
      "loss: 2.668671 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658219, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 45]\n",
      "loss: 2.668672 [0/28137]\n",
      "loss: 2.669560 [1280/28137]\n",
      "loss: 2.661065 [2560/28137]\n",
      "loss: 2.645561 [3840/28137]\n",
      "loss: 2.660956 [5120/28137]\n",
      "loss: 2.645559 [6400/28137]\n",
      "loss: 2.637852 [7680/28137]\n",
      "loss: 2.637847 [8960/28137]\n",
      "loss: 2.660968 [10240/28137]\n",
      "loss: 2.660920 [11520/28137]\n",
      "loss: 2.630152 [12800/28137]\n",
      "loss: 2.661048 [14080/28137]\n",
      "loss: 2.645551 [15360/28137]\n",
      "loss: 2.645532 [16640/28137]\n",
      "loss: 2.645546 [17920/28137]\n",
      "loss: 2.653239 [19200/28137]\n",
      "loss: 2.645531 [20480/28137]\n",
      "loss: 2.645540 [21760/28137]\n",
      "loss: 2.668631 [23040/28137]\n",
      "loss: 2.645535 [24320/28137]\n",
      "loss: 2.661055 [25600/28137]\n",
      "loss: 2.645580 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658372, Accuracy: 3618/3725 (97.1%)\n",
      "\n",
      "\n",
      "[Epoch 46]\n",
      "loss: 2.660935 [0/28137]\n",
      "loss: 2.653239 [1280/28137]\n",
      "loss: 2.645538 [2560/28137]\n",
      "loss: 2.645534 [3840/28137]\n",
      "loss: 2.699460 [5120/28137]\n",
      "loss: 2.653348 [6400/28137]\n",
      "loss: 2.660970 [7680/28137]\n",
      "loss: 2.660931 [8960/28137]\n",
      "loss: 2.660935 [10240/28137]\n",
      "loss: 2.668732 [11520/28137]\n",
      "loss: 2.645538 [12800/28137]\n",
      "loss: 2.637839 [14080/28137]\n",
      "loss: 2.637842 [15360/28137]\n",
      "loss: 2.668609 [16640/28137]\n",
      "loss: 2.645538 [17920/28137]\n",
      "loss: 2.676317 [19200/28137]\n",
      "loss: 2.660938 [20480/28137]\n",
      "loss: 2.660963 [21760/28137]\n",
      "loss: 2.676316 [23040/28137]\n",
      "loss: 2.653239 [24320/28137]\n",
      "loss: 2.645550 [25600/28137]\n",
      "loss: 2.684052 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658316, Accuracy: 3618/3725 (97.1%)\n",
      "\n",
      "\n",
      "[Epoch 47]\n",
      "loss: 2.676335 [0/28137]\n",
      "loss: 2.653227 [1280/28137]\n",
      "loss: 2.684001 [2560/28137]\n",
      "loss: 2.645536 [3840/28137]\n",
      "loss: 2.676427 [5120/28137]\n",
      "loss: 2.637845 [6400/28137]\n",
      "loss: 2.684075 [7680/28137]\n",
      "loss: 2.660917 [8960/28137]\n",
      "loss: 2.653229 [10240/28137]\n",
      "loss: 2.668631 [11520/28137]\n",
      "loss: 2.653226 [12800/28137]\n",
      "loss: 2.653273 [14080/28137]\n",
      "loss: 2.684174 [15360/28137]\n",
      "loss: 2.676317 [16640/28137]\n",
      "loss: 2.668618 [17920/28137]\n",
      "loss: 2.676315 [19200/28137]\n",
      "loss: 2.668763 [20480/28137]\n",
      "loss: 2.645537 [21760/28137]\n",
      "loss: 2.660950 [23040/28137]\n",
      "loss: 2.699512 [24320/28137]\n",
      "loss: 2.684037 [25600/28137]\n",
      "loss: 2.653234 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658292, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 48]\n",
      "loss: 2.668618 [0/28137]\n",
      "loss: 2.653224 [1280/28137]\n",
      "loss: 2.660922 [2560/28137]\n",
      "loss: 2.653235 [3840/28137]\n",
      "loss: 2.637846 [5120/28137]\n",
      "loss: 2.684023 [6400/28137]\n",
      "loss: 2.645551 [7680/28137]\n",
      "loss: 2.668656 [8960/28137]\n",
      "loss: 2.660953 [10240/28137]\n",
      "loss: 2.660930 [11520/28137]\n",
      "loss: 2.653250 [12800/28137]\n",
      "loss: 2.676307 [14080/28137]\n",
      "loss: 2.676406 [15360/28137]\n",
      "loss: 2.653231 [16640/28137]\n",
      "loss: 2.668730 [17920/28137]\n",
      "loss: 2.645544 [19200/28137]\n",
      "loss: 2.653227 [20480/28137]\n",
      "loss: 2.684018 [21760/28137]\n",
      "loss: 2.653354 [23040/28137]\n",
      "loss: 2.668789 [24320/28137]\n",
      "loss: 2.668606 [25600/28137]\n",
      "loss: 2.645534 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658249, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 49]\n",
      "loss: 2.645546 [0/28137]\n",
      "loss: 2.637861 [1280/28137]\n",
      "loss: 2.645554 [2560/28137]\n",
      "loss: 2.645534 [3840/28137]\n",
      "loss: 2.676420 [5120/28137]\n",
      "loss: 2.676343 [6400/28137]\n",
      "loss: 2.653284 [7680/28137]\n",
      "loss: 2.676357 [8960/28137]\n",
      "loss: 2.684015 [10240/28137]\n",
      "loss: 2.661154 [11520/28137]\n",
      "loss: 2.653236 [12800/28137]\n",
      "loss: 2.660952 [14080/28137]\n",
      "loss: 2.668616 [15360/28137]\n",
      "loss: 2.653226 [16640/28137]\n",
      "loss: 2.668659 [17920/28137]\n",
      "loss: 2.645531 [19200/28137]\n",
      "loss: 2.653234 [20480/28137]\n",
      "loss: 2.637904 [21760/28137]\n",
      "loss: 2.668607 [23040/28137]\n",
      "loss: 2.637843 [24320/28137]\n",
      "loss: 2.645545 [25600/28137]\n",
      "loss: 2.653229 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658231, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 50]\n",
      "loss: 2.653228 [0/28137]\n",
      "loss: 2.660939 [1280/28137]\n",
      "loss: 2.645532 [2560/28137]\n",
      "loss: 2.637864 [3840/28137]\n",
      "loss: 2.653343 [5120/28137]\n",
      "loss: 2.645531 [6400/28137]\n",
      "loss: 2.676317 [7680/28137]\n",
      "loss: 2.660927 [8960/28137]\n",
      "loss: 2.645537 [10240/28137]\n",
      "loss: 2.637839 [11520/28137]\n",
      "loss: 2.660958 [12800/28137]\n",
      "loss: 2.653225 [14080/28137]\n",
      "loss: 2.645537 [15360/28137]\n",
      "loss: 2.684010 [16640/28137]\n",
      "loss: 2.630153 [17920/28137]\n",
      "loss: 2.676298 [19200/28137]\n",
      "loss: 2.660923 [20480/28137]\n",
      "loss: 2.653233 [21760/28137]\n",
      "loss: 2.645538 [23040/28137]\n",
      "loss: 2.661037 [24320/28137]\n",
      "loss: 2.637836 [25600/28137]\n",
      "loss: 2.653286 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658218, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 51]\n",
      "loss: 2.676315 [0/28137]\n",
      "loss: 2.661035 [1280/28137]\n",
      "loss: 2.668624 [2560/28137]\n",
      "loss: 2.676310 [3840/28137]\n",
      "loss: 2.668639 [5120/28137]\n",
      "loss: 2.676329 [6400/28137]\n",
      "loss: 2.645532 [7680/28137]\n",
      "loss: 2.653231 [8960/28137]\n",
      "loss: 2.660924 [10240/28137]\n",
      "loss: 2.684008 [11520/28137]\n",
      "loss: 2.645572 [12800/28137]\n",
      "loss: 2.660956 [14080/28137]\n",
      "loss: 2.660917 [15360/28137]\n",
      "loss: 2.661037 [16640/28137]\n",
      "loss: 2.653224 [17920/28137]\n",
      "loss: 2.653225 [19200/28137]\n",
      "loss: 2.653223 [20480/28137]\n",
      "loss: 2.660933 [21760/28137]\n",
      "loss: 2.645533 [23040/28137]\n",
      "loss: 2.637841 [24320/28137]\n",
      "loss: 2.676317 [25600/28137]\n",
      "loss: 2.653293 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658209, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 52]\n",
      "loss: 2.653241 [0/28137]\n",
      "loss: 2.645553 [1280/28137]\n",
      "loss: 2.645531 [2560/28137]\n",
      "loss: 2.653231 [3840/28137]\n",
      "loss: 2.683998 [5120/28137]\n",
      "loss: 2.660919 [6400/28137]\n",
      "loss: 2.683996 [7680/28137]\n",
      "loss: 2.653234 [8960/28137]\n",
      "loss: 2.653220 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.653229 [12800/28137]\n",
      "loss: 2.630146 [14080/28137]\n",
      "loss: 2.714837 [15360/28137]\n",
      "loss: 2.707205 [16640/28137]\n",
      "loss: 2.637838 [17920/28137]\n",
      "loss: 2.645540 [19200/28137]\n",
      "loss: 2.653223 [20480/28137]\n",
      "loss: 2.653221 [21760/28137]\n",
      "loss: 2.660919 [23040/28137]\n",
      "loss: 2.676307 [24320/28137]\n",
      "loss: 2.637848 [25600/28137]\n",
      "loss: 2.668628 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658178, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 53]\n",
      "loss: 2.668628 [0/28137]\n",
      "loss: 2.645541 [1280/28137]\n",
      "loss: 2.645535 [2560/28137]\n",
      "loss: 2.645544 [3840/28137]\n",
      "loss: 2.645538 [5120/28137]\n",
      "loss: 2.653259 [6400/28137]\n",
      "loss: 2.668749 [7680/28137]\n",
      "loss: 2.684116 [8960/28137]\n",
      "loss: 2.645566 [10240/28137]\n",
      "loss: 2.661049 [11520/28137]\n",
      "loss: 2.668621 [12800/28137]\n",
      "loss: 2.653260 [14080/28137]\n",
      "loss: 2.660930 [15360/28137]\n",
      "loss: 2.668610 [16640/28137]\n",
      "loss: 2.637836 [17920/28137]\n",
      "loss: 2.653229 [19200/28137]\n",
      "loss: 2.653222 [20480/28137]\n",
      "loss: 2.653223 [21760/28137]\n",
      "loss: 2.676302 [23040/28137]\n",
      "loss: 2.645533 [24320/28137]\n",
      "loss: 2.707086 [25600/28137]\n",
      "loss: 2.653315 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658151, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 54]\n",
      "loss: 2.653222 [0/28137]\n",
      "loss: 2.645529 [1280/28137]\n",
      "loss: 2.653222 [2560/28137]\n",
      "loss: 2.714776 [3840/28137]\n",
      "loss: 2.691697 [5120/28137]\n",
      "loss: 2.645529 [6400/28137]\n",
      "loss: 2.645530 [7680/28137]\n",
      "loss: 2.676298 [8960/28137]\n",
      "loss: 2.691698 [10240/28137]\n",
      "loss: 2.668609 [11520/28137]\n",
      "loss: 2.660927 [12800/28137]\n",
      "loss: 2.645535 [14080/28137]\n",
      "loss: 2.637838 [15360/28137]\n",
      "loss: 2.653219 [16640/28137]\n",
      "loss: 2.660921 [17920/28137]\n",
      "loss: 2.630146 [19200/28137]\n",
      "loss: 2.645538 [20480/28137]\n",
      "loss: 2.653228 [21760/28137]\n",
      "loss: 2.660916 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.645548 [25600/28137]\n",
      "loss: 2.668602 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658131, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 55]\n",
      "loss: 2.653225 [0/28137]\n",
      "loss: 2.683989 [1280/28137]\n",
      "loss: 2.676300 [2560/28137]\n",
      "loss: 2.676313 [3840/28137]\n",
      "loss: 2.637843 [5120/28137]\n",
      "loss: 2.668607 [6400/28137]\n",
      "loss: 2.660937 [7680/28137]\n",
      "loss: 2.676316 [8960/28137]\n",
      "loss: 2.645555 [10240/28137]\n",
      "loss: 2.684019 [11520/28137]\n",
      "loss: 2.660921 [12800/28137]\n",
      "loss: 2.660916 [14080/28137]\n",
      "loss: 2.660915 [15360/28137]\n",
      "loss: 2.676428 [16640/28137]\n",
      "loss: 2.653228 [17920/28137]\n",
      "loss: 2.653234 [19200/28137]\n",
      "loss: 2.707212 [20480/28137]\n",
      "loss: 2.668612 [21760/28137]\n",
      "loss: 2.653254 [23040/28137]\n",
      "loss: 2.691812 [24320/28137]\n",
      "loss: 2.668605 [25600/28137]\n",
      "loss: 2.637841 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658106, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 56]\n",
      "loss: 2.653221 [0/28137]\n",
      "loss: 2.661039 [1280/28137]\n",
      "loss: 2.637862 [2560/28137]\n",
      "loss: 2.668619 [3840/28137]\n",
      "loss: 2.653225 [5120/28137]\n",
      "loss: 2.683990 [6400/28137]\n",
      "loss: 2.653220 [7680/28137]\n",
      "loss: 2.668608 [8960/28137]\n",
      "loss: 2.668612 [10240/28137]\n",
      "loss: 2.676309 [11520/28137]\n",
      "loss: 2.637837 [12800/28137]\n",
      "loss: 2.660918 [14080/28137]\n",
      "loss: 2.645540 [15360/28137]\n",
      "loss: 2.660926 [16640/28137]\n",
      "loss: 2.668734 [17920/28137]\n",
      "loss: 2.653226 [19200/28137]\n",
      "loss: 2.653226 [20480/28137]\n",
      "loss: 2.691687 [21760/28137]\n",
      "loss: 2.668621 [23040/28137]\n",
      "loss: 2.653231 [24320/28137]\n",
      "loss: 2.637842 [25600/28137]\n",
      "loss: 2.637838 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658090, Accuracy: 3619/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 57]\n",
      "loss: 2.676299 [0/28137]\n",
      "loss: 2.637838 [1280/28137]\n",
      "loss: 2.645535 [2560/28137]\n",
      "loss: 2.660914 [3840/28137]\n",
      "loss: 2.653221 [5120/28137]\n",
      "loss: 2.676318 [6400/28137]\n",
      "loss: 2.653225 [7680/28137]\n",
      "loss: 2.668626 [8960/28137]\n",
      "loss: 2.668613 [10240/28137]\n",
      "loss: 2.660924 [11520/28137]\n",
      "loss: 2.653225 [12800/28137]\n",
      "loss: 2.660914 [14080/28137]\n",
      "loss: 2.653224 [15360/28137]\n",
      "loss: 2.653225 [16640/28137]\n",
      "loss: 2.660923 [17920/28137]\n",
      "loss: 2.660926 [19200/28137]\n",
      "loss: 2.653222 [20480/28137]\n",
      "loss: 2.653222 [21760/28137]\n",
      "loss: 2.637839 [23040/28137]\n",
      "loss: 2.645529 [24320/28137]\n",
      "loss: 2.676300 [25600/28137]\n",
      "loss: 2.691741 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658068, Accuracy: 3620/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 58]\n",
      "loss: 2.661044 [0/28137]\n",
      "loss: 2.653229 [1280/28137]\n",
      "loss: 2.645540 [2560/28137]\n",
      "loss: 2.645544 [3840/28137]\n",
      "loss: 2.645532 [5120/28137]\n",
      "loss: 2.660971 [6400/28137]\n",
      "loss: 2.653232 [7680/28137]\n",
      "loss: 2.653226 [8960/28137]\n",
      "loss: 2.660920 [10240/28137]\n",
      "loss: 2.660929 [11520/28137]\n",
      "loss: 2.645530 [12800/28137]\n",
      "loss: 2.653222 [14080/28137]\n",
      "loss: 2.668614 [15360/28137]\n",
      "loss: 2.637838 [16640/28137]\n",
      "loss: 2.660923 [17920/28137]\n",
      "loss: 2.676297 [19200/28137]\n",
      "loss: 2.637837 [20480/28137]\n",
      "loss: 2.653228 [21760/28137]\n",
      "loss: 2.645534 [23040/28137]\n",
      "loss: 2.660917 [24320/28137]\n",
      "loss: 2.645530 [25600/28137]\n",
      "loss: 2.684012 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658076, Accuracy: 3620/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 59]\n",
      "loss: 2.660919 [0/28137]\n",
      "loss: 2.660920 [1280/28137]\n",
      "loss: 2.653239 [2560/28137]\n",
      "loss: 2.668608 [3840/28137]\n",
      "loss: 2.653224 [5120/28137]\n",
      "loss: 2.637834 [6400/28137]\n",
      "loss: 2.653224 [7680/28137]\n",
      "loss: 2.645529 [8960/28137]\n",
      "loss: 2.653226 [10240/28137]\n",
      "loss: 2.637840 [11520/28137]\n",
      "loss: 2.645531 [12800/28137]\n",
      "loss: 2.668607 [14080/28137]\n",
      "loss: 2.660932 [15360/28137]\n",
      "loss: 2.660914 [16640/28137]\n",
      "loss: 2.653221 [17920/28137]\n",
      "loss: 2.668606 [19200/28137]\n",
      "loss: 2.637840 [20480/28137]\n",
      "loss: 2.684109 [21760/28137]\n",
      "loss: 2.645532 [23040/28137]\n",
      "loss: 2.684004 [24320/28137]\n",
      "loss: 2.637838 [25600/28137]\n",
      "loss: 2.676299 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658042, Accuracy: 3620/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 60]\n",
      "loss: 2.676326 [0/28137]\n",
      "loss: 2.660915 [1280/28137]\n",
      "loss: 2.653229 [2560/28137]\n",
      "loss: 2.668868 [3840/28137]\n",
      "loss: 2.645536 [5120/28137]\n",
      "loss: 2.668614 [6400/28137]\n",
      "loss: 2.637837 [7680/28137]\n",
      "loss: 2.676308 [8960/28137]\n",
      "loss: 2.683986 [10240/28137]\n",
      "loss: 2.660913 [11520/28137]\n",
      "loss: 2.653238 [12800/28137]\n",
      "loss: 2.668600 [14080/28137]\n",
      "loss: 2.653222 [15360/28137]\n",
      "loss: 2.683990 [16640/28137]\n",
      "loss: 2.660914 [17920/28137]\n",
      "loss: 2.653223 [19200/28137]\n",
      "loss: 2.653222 [20480/28137]\n",
      "loss: 2.645527 [21760/28137]\n",
      "loss: 2.668722 [23040/28137]\n",
      "loss: 2.660912 [24320/28137]\n",
      "loss: 2.660916 [25600/28137]\n",
      "loss: 2.637837 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658075, Accuracy: 3620/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 61]\n",
      "loss: 2.653224 [0/28137]\n",
      "loss: 2.683989 [1280/28137]\n",
      "loss: 2.637840 [2560/28137]\n",
      "loss: 2.653221 [3840/28137]\n",
      "loss: 2.676291 [5120/28137]\n",
      "loss: 2.691676 [6400/28137]\n",
      "loss: 2.653225 [7680/28137]\n",
      "loss: 2.668610 [8960/28137]\n",
      "loss: 2.645534 [10240/28137]\n",
      "loss: 2.668619 [11520/28137]\n",
      "loss: 2.653340 [12800/28137]\n",
      "loss: 2.684044 [14080/28137]\n",
      "loss: 2.676297 [15360/28137]\n",
      "loss: 2.668600 [16640/28137]\n",
      "loss: 2.637840 [17920/28137]\n",
      "loss: 2.668606 [19200/28137]\n",
      "loss: 2.676299 [20480/28137]\n",
      "loss: 2.637838 [21760/28137]\n",
      "loss: 2.660923 [23040/28137]\n",
      "loss: 2.668607 [24320/28137]\n",
      "loss: 2.645534 [25600/28137]\n",
      "loss: 2.653221 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658076, Accuracy: 3620/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 62]\n",
      "loss: 2.676299 [0/28137]\n",
      "loss: 2.660915 [1280/28137]\n",
      "loss: 2.653223 [2560/28137]\n",
      "loss: 2.653219 [3840/28137]\n",
      "loss: 2.676296 [5120/28137]\n",
      "loss: 2.653225 [6400/28137]\n",
      "loss: 2.653220 [7680/28137]\n",
      "loss: 2.676297 [8960/28137]\n",
      "loss: 2.668618 [10240/28137]\n",
      "loss: 2.676300 [11520/28137]\n",
      "loss: 2.691807 [12800/28137]\n",
      "loss: 2.645529 [14080/28137]\n",
      "loss: 2.660912 [15360/28137]\n",
      "loss: 2.645527 [16640/28137]\n",
      "loss: 2.637845 [17920/28137]\n",
      "loss: 2.668632 [19200/28137]\n",
      "loss: 2.637841 [20480/28137]\n",
      "loss: 2.660893 [21760/28137]\n",
      "loss: 2.653211 [23040/28137]\n",
      "loss: 2.653207 [24320/28137]\n",
      "loss: 2.637837 [25600/28137]\n",
      "loss: 2.683951 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.658039, Accuracy: 3620/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 63]\n",
      "loss: 2.660906 [0/28137]\n",
      "loss: 2.660898 [1280/28137]\n",
      "loss: 2.660900 [2560/28137]\n",
      "loss: 2.645522 [3840/28137]\n",
      "loss: 2.653209 [5120/28137]\n",
      "loss: 2.660894 [6400/28137]\n",
      "loss: 2.683963 [7680/28137]\n",
      "loss: 2.691651 [8960/28137]\n",
      "loss: 2.645524 [10240/28137]\n",
      "loss: 2.645520 [11520/28137]\n",
      "loss: 2.668580 [12800/28137]\n",
      "loss: 2.645525 [14080/28137]\n",
      "loss: 2.668578 [15360/28137]\n",
      "loss: 2.653220 [16640/28137]\n",
      "loss: 2.660898 [17920/28137]\n",
      "loss: 2.660897 [19200/28137]\n",
      "loss: 2.645530 [20480/28137]\n",
      "loss: 2.637833 [21760/28137]\n",
      "loss: 2.668581 [23040/28137]\n",
      "loss: 2.691660 [24320/28137]\n",
      "loss: 2.645526 [25600/28137]\n",
      "loss: 2.660913 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.657989, Accuracy: 3620/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 64]\n",
      "loss: 2.637841 [0/28137]\n",
      "loss: 2.645520 [1280/28137]\n",
      "loss: 2.645521 [2560/28137]\n",
      "loss: 2.676264 [3840/28137]\n",
      "loss: 2.676271 [5120/28137]\n",
      "loss: 2.645519 [6400/28137]\n",
      "loss: 2.645517 [7680/28137]\n",
      "loss: 2.637834 [8960/28137]\n",
      "loss: 2.660900 [10240/28137]\n",
      "loss: 2.653206 [11520/28137]\n",
      "loss: 2.668710 [12800/28137]\n",
      "loss: 2.668709 [14080/28137]\n",
      "loss: 2.653205 [15360/28137]\n",
      "loss: 2.653206 [16640/28137]\n",
      "loss: 2.660891 [17920/28137]\n",
      "loss: 2.637831 [19200/28137]\n",
      "loss: 2.637833 [20480/28137]\n",
      "loss: 2.668577 [21760/28137]\n",
      "loss: 2.653202 [23040/28137]\n",
      "loss: 2.676270 [24320/28137]\n",
      "loss: 2.645519 [25600/28137]\n",
      "loss: 2.653207 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.657964, Accuracy: 3620/3725 (97.2%)\n",
      "\n",
      "\n",
      "[Epoch 65]\n",
      "loss: 2.668576 [0/28137]\n",
      "loss: 2.637833 [1280/28137]\n",
      "loss: 2.631779 [2560/28137]\n",
      "loss: 2.630235 [3840/28137]\n",
      "loss: 2.639145 [5120/28137]\n",
      "loss: 2.660051 [6400/28137]\n",
      "loss: 2.647356 [7680/28137]\n",
      "loss: 2.646243 [8960/28137]\n",
      "loss: 2.634435 [10240/28137]\n",
      "loss: 2.640015 [11520/28137]\n",
      "loss: 2.633127 [12800/28137]\n",
      "loss: 2.637985 [14080/28137]\n",
      "loss: 2.630329 [15360/28137]\n",
      "loss: 2.630147 [16640/28137]\n",
      "loss: 2.630177 [17920/28137]\n",
      "loss: 2.630153 [19200/28137]\n",
      "loss: 2.630154 [20480/28137]\n",
      "loss: 2.637959 [21760/28137]\n",
      "loss: 2.638124 [23040/28137]\n",
      "loss: 2.630163 [24320/28137]\n",
      "loss: 2.630172 [25600/28137]\n",
      "loss: 2.630190 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.632405, Accuracy: 3717/3725 (99.8%)\n",
      "\n",
      "\n",
      "[Epoch 66]\n",
      "loss: 2.638263 [0/28137]\n",
      "loss: 2.638282 [1280/28137]\n",
      "loss: 2.630613 [2560/28137]\n",
      "loss: 2.631153 [3840/28137]\n",
      "loss: 2.630150 [5120/28137]\n",
      "loss: 2.630153 [6400/28137]\n",
      "loss: 2.630226 [7680/28137]\n",
      "loss: 2.637313 [8960/28137]\n",
      "loss: 2.637563 [10240/28137]\n",
      "loss: 2.634196 [11520/28137]\n",
      "loss: 2.630152 [12800/28137]\n",
      "loss: 2.630147 [14080/28137]\n",
      "loss: 2.637986 [15360/28137]\n",
      "loss: 2.630167 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630147 [19200/28137]\n",
      "loss: 2.630165 [20480/28137]\n",
      "loss: 2.630147 [21760/28137]\n",
      "loss: 2.630152 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630152 [25600/28137]\n",
      "loss: 2.630148 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.632561, Accuracy: 3716/3725 (99.8%)\n",
      "\n",
      "\n",
      "[Epoch 67]\n",
      "loss: 2.630220 [0/28137]\n",
      "loss: 2.630149 [1280/28137]\n",
      "loss: 2.634503 [2560/28137]\n",
      "loss: 2.631107 [3840/28137]\n",
      "loss: 2.637945 [5120/28137]\n",
      "loss: 2.630148 [6400/28137]\n",
      "loss: 2.630149 [7680/28137]\n",
      "loss: 2.630146 [8960/28137]\n",
      "loss: 2.630182 [10240/28137]\n",
      "loss: 2.637963 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630147 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630147 [16640/28137]\n",
      "loss: 2.630147 [17920/28137]\n",
      "loss: 2.630165 [19200/28137]\n",
      "loss: 2.630148 [20480/28137]\n",
      "loss: 2.630153 [21760/28137]\n",
      "loss: 2.630287 [23040/28137]\n",
      "loss: 2.630153 [24320/28137]\n",
      "loss: 2.630147 [25600/28137]\n",
      "loss: 2.630146 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.631468, Accuracy: 3720/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 68]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630147 [1280/28137]\n",
      "loss: 2.630146 [2560/28137]\n",
      "loss: 2.630146 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630148 [6400/28137]\n",
      "loss: 2.630149 [7680/28137]\n",
      "loss: 2.637960 [8960/28137]\n",
      "loss: 2.630146 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630149 [14080/28137]\n",
      "loss: 2.630147 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630239 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630150 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.631299, Accuracy: 3721/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 69]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630146 [1280/28137]\n",
      "loss: 2.630146 [2560/28137]\n",
      "loss: 2.630146 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630146 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630147 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630146 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630147 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.637957 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.631266, Accuracy: 3721/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 70]\n",
      "loss: 2.630146 [0/28137]\n",
      "loss: 2.637914 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630147 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630149 [14080/28137]\n",
      "loss: 2.630147 [15360/28137]\n",
      "loss: 2.637957 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630146 [24320/28137]\n",
      "loss: 2.630147 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.631247, Accuracy: 3721/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 71]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630146 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630146 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630146 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.637959 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.637957 [12800/28137]\n",
      "loss: 2.630148 [14080/28137]\n",
      "loss: 2.630151 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.637958 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630146 [21760/28137]\n",
      "loss: 2.630214 [23040/28137]\n",
      "loss: 2.630268 [24320/28137]\n",
      "loss: 2.630167 [25600/28137]\n",
      "loss: 2.630147 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.631139, Accuracy: 3722/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 72]\n",
      "loss: 2.630149 [0/28137]\n",
      "loss: 2.637959 [1280/28137]\n",
      "loss: 2.630147 [2560/28137]\n",
      "loss: 2.630206 [3840/28137]\n",
      "loss: 2.630149 [5120/28137]\n",
      "loss: 2.630148 [6400/28137]\n",
      "loss: 2.637958 [7680/28137]\n",
      "loss: 2.630148 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630149 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.637957 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630146 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630146 [23040/28137]\n",
      "loss: 2.630147 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630150 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.631016, Accuracy: 3722/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 73]\n",
      "loss: 2.630154 [0/28137]\n",
      "loss: 2.630146 [1280/28137]\n",
      "loss: 2.630172 [2560/28137]\n",
      "loss: 2.630199 [3840/28137]\n",
      "loss: 2.636636 [5120/28137]\n",
      "loss: 2.630191 [6400/28137]\n",
      "loss: 2.630217 [7680/28137]\n",
      "loss: 2.632457 [8960/28137]\n",
      "loss: 2.630286 [10240/28137]\n",
      "loss: 2.630163 [11520/28137]\n",
      "loss: 2.630900 [12800/28137]\n",
      "loss: 2.630163 [14080/28137]\n",
      "loss: 2.630148 [15360/28137]\n",
      "loss: 2.630163 [16640/28137]\n",
      "loss: 2.630149 [17920/28137]\n",
      "loss: 2.630802 [19200/28137]\n",
      "loss: 2.636962 [20480/28137]\n",
      "loss: 2.630199 [21760/28137]\n",
      "loss: 2.637973 [23040/28137]\n",
      "loss: 2.637969 [24320/28137]\n",
      "loss: 2.630183 [25600/28137]\n",
      "loss: 2.631049 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.632439, Accuracy: 3716/3725 (99.8%)\n",
      "\n",
      "\n",
      "[Epoch 74]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630155 [1280/28137]\n",
      "loss: 2.630200 [2560/28137]\n",
      "loss: 2.633432 [3840/28137]\n",
      "loss: 2.630162 [5120/28137]\n",
      "loss: 2.630316 [6400/28137]\n",
      "loss: 2.637968 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630173 [10240/28137]\n",
      "loss: 2.637967 [11520/28137]\n",
      "loss: 2.630157 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630156 [19200/28137]\n",
      "loss: 2.630147 [20480/28137]\n",
      "loss: 2.630148 [21760/28137]\n",
      "loss: 2.630148 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630149 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630914, Accuracy: 3722/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 75]\n",
      "loss: 2.630146 [0/28137]\n",
      "loss: 2.630402 [1280/28137]\n",
      "loss: 2.630146 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630147 [5120/28137]\n",
      "loss: 2.630154 [6400/28137]\n",
      "loss: 2.630192 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630147 [10240/28137]\n",
      "loss: 2.645771 [11520/28137]\n",
      "loss: 2.630146 [12800/28137]\n",
      "loss: 2.645772 [14080/28137]\n",
      "loss: 2.630146 [15360/28137]\n",
      "loss: 2.630148 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630146 [20480/28137]\n",
      "loss: 2.630151 [21760/28137]\n",
      "loss: 2.630148 [23040/28137]\n",
      "loss: 2.630147 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630187 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.632059, Accuracy: 3718/3725 (99.8%)\n",
      "\n",
      "\n",
      "[Epoch 76]\n",
      "loss: 2.630170 [0/28137]\n",
      "loss: 2.630151 [1280/28137]\n",
      "loss: 2.630155 [2560/28137]\n",
      "loss: 2.630147 [3840/28137]\n",
      "loss: 2.630146 [5120/28137]\n",
      "loss: 2.630294 [6400/28137]\n",
      "loss: 2.630147 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630339 [10240/28137]\n",
      "loss: 2.630148 [11520/28137]\n",
      "loss: 2.630151 [12800/28137]\n",
      "loss: 2.637504 [14080/28137]\n",
      "loss: 2.630147 [15360/28137]\n",
      "loss: 2.630184 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.637652 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630186 [21760/28137]\n",
      "loss: 2.630147 [23040/28137]\n",
      "loss: 2.641217 [24320/28137]\n",
      "loss: 2.630170 [25600/28137]\n",
      "loss: 2.630150 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.631891, Accuracy: 3719/3725 (99.8%)\n",
      "\n",
      "\n",
      "[Epoch 77]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630146 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630153 [5120/28137]\n",
      "loss: 2.637965 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630147 [8960/28137]\n",
      "loss: 2.630148 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630151 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630199 [15360/28137]\n",
      "loss: 2.630146 [16640/28137]\n",
      "loss: 2.630146 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630146 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630170 [24320/28137]\n",
      "loss: 2.630174 [25600/28137]\n",
      "loss: 2.630149 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630967, Accuracy: 3722/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 78]\n",
      "loss: 2.630147 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630148 [2560/28137]\n",
      "loss: 2.630150 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630146 [12800/28137]\n",
      "loss: 2.630146 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630146 [17920/28137]\n",
      "loss: 2.635555 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630147 [21760/28137]\n",
      "loss: 2.630237 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630146 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630882, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 79]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630148 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630150 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630148 [12800/28137]\n",
      "loss: 2.630146 [14080/28137]\n",
      "loss: 2.630146 [15360/28137]\n",
      "loss: 2.630148 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630693, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 80]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630146 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630147 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630146 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630146 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630147 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630689, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 81]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630146 [2560/28137]\n",
      "loss: 2.630146 [3840/28137]\n",
      "loss: 2.630146 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.637957 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630147 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.637957 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630687, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 82]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.637957 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.645770 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.637957 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630686, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 83]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.637957 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630146 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630685, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 84]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.645771 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.637957 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630685, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 85]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.637957 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630146 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.637957 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630684, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 86]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630146 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.637957 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630146 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630684, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 87]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630146 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630683, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 88]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.637957 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.637957 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630683, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 89]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.637957 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.637957 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.637957 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630683, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 90]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.637957 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.637957 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630683, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 91]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.637957 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.637957 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.637957 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630683, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 92]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.637957 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.637958 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.637957 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630683, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 93]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.637958 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630682, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 94]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.637957 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630682, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 95]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.637957 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630682, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 96]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.637957 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630682, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 97]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630682, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 98]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.637957 [3840/28137]\n",
      "loss: 2.637957 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.637957 [12800/28137]\n",
      "loss: 2.637957 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.637957 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630682, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 99]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.637957 [14080/28137]\n",
      "loss: 2.630145 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.630145 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630682, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "[Epoch 100]\n",
      "loss: 2.630145 [0/28137]\n",
      "loss: 2.630145 [1280/28137]\n",
      "loss: 2.630145 [2560/28137]\n",
      "loss: 2.630145 [3840/28137]\n",
      "loss: 2.630145 [5120/28137]\n",
      "loss: 2.630145 [6400/28137]\n",
      "loss: 2.630145 [7680/28137]\n",
      "loss: 2.630145 [8960/28137]\n",
      "loss: 2.630145 [10240/28137]\n",
      "loss: 2.630145 [11520/28137]\n",
      "loss: 2.630145 [12800/28137]\n",
      "loss: 2.630145 [14080/28137]\n",
      "loss: 2.637957 [15360/28137]\n",
      "loss: 2.630145 [16640/28137]\n",
      "loss: 2.630145 [17920/28137]\n",
      "loss: 2.630145 [19200/28137]\n",
      "loss: 2.630145 [20480/28137]\n",
      "loss: 2.630145 [21760/28137]\n",
      "loss: 2.637957 [23040/28137]\n",
      "loss: 2.630145 [24320/28137]\n",
      "loss: 2.630145 [25600/28137]\n",
      "loss: 2.630145 [26880/28137]\n",
      "\n",
      "***Validation Result***\n",
      "Average loss: 2.630682, Accuracy: 3723/3725 (99.9%)\n",
      "\n",
      "\n",
      "Done!\n",
      "Training time: 2059.6332\n"
     ]
    }
   ],
   "source": [
    "## Training Operation ##\n",
    "start = time.time()\n",
    "for t in range(epochs):\n",
    "  print(f\"\\n[Epoch {t+1}]\")\n",
    "  train(train_dataloader, model, loss_fn, optimizer)\n",
    "  validation(model, val_dataloader)\n",
    "print(\"\\nDone!\")\n",
    "end = time.time()\n",
    "\n",
    "print(\"Training time: {:.4f}\".format(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Test Result***\n",
      "Average loss: 2.630718, Accuracy: 3558/3560 (99.9%)\n",
      "\n",
      "Test time: 4.2049\n"
     ]
    }
   ],
   "source": [
    "## Test operation ##\n",
    "start = time.time()\n",
    "test(model, test_dataloader)\n",
    "end = time.time()\n",
    "print(\"Test time: {:.4f}\".format(end-start))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "torch.save(model, './model_231204.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
