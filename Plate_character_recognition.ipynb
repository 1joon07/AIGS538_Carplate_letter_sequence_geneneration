{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, CenterCrop, Grayscale\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import dataset as data\n",
    "import model as md\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} device\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "Batch_size = 4\n",
    "Optimizer_type = 'SGD'\n",
    "Learning_rate = 1e-3\n",
    "Weight_decay = 0\n",
    "epochs = 15"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def Sample_visualize(dataloader) :\n",
    "  ## Visualize some preprocess images\n",
    "  ##\n",
    "  ## Input : dataloader of image dataset\n",
    "  ## Output : image plots\n",
    "\n",
    " dataiter = iter(dataloader)\n",
    " images, labels = next(dataiter)\n",
    " images = images.numpy()\n",
    " for i in range (2) :\n",
    "   plt.subplot(1,4,i+1)\n",
    "   plt.imshow(images[i].reshape(32,96), cmap='gray')\n",
    "   plt.xticks([])\n",
    "   plt.yticks([])\n",
    "\n",
    "   print(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_dataset = data.LicensePlateDataset(directory='./CNN_generated_dataset', label_file='./CNN_generated_dataset/Labels.csv')\n",
    "train_loader = DataLoader(train_dataset, batch_size=Batch_size, shuffle=True)\n",
    "\n",
    "# val_dataset = data.LicensePlateDataset(directory='./CNN_generated_dataset_val', label_file='./CNN_generated_dataset_val/Labels.csv')\n",
    "# val_loader = DataLoader(val_dataset, batch_size=Batch_size, shuffle=True)\n",
    "#\n",
    "# test_dataset = data.LicensePlateDataset(directory='./CNN_generated_dataset_test', label_file='./CNN_generated_dataset_test/Labels.csv')\n",
    "# test_loader = DataLoader(test_dataset, batch_size=Batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([23,  4, 18, 34]), tensor([ 4, 31, 25, 12]), tensor([12,  5, 17,  8]), tensor([ 9, 19,  7, 29]), tensor([32, 32, 21,  8]), tensor([28, 35, 30, 25]), tensor([ 6, 35, 30, 15]), tensor([16, 35, 22, 33]), tensor([ 7, 35, 28, 35]), tensor([15, 35,  4, 35]), tensor([12, 35, 35, 35])]\n",
      "[tensor([23,  4, 18, 34]), tensor([ 4, 31, 25, 12]), tensor([12,  5, 17,  8]), tensor([ 9, 19,  7, 29]), tensor([32, 32, 21,  8]), tensor([28, 35, 30, 25]), tensor([ 6, 35, 30, 15]), tensor([16, 35, 22, 33]), tensor([ 7, 35, 28, 35]), tensor([15, 35,  4, 35]), tensor([12, 35, 35, 35])]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sample_visualize(train_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 48, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(48, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=3360, out_features=50, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=50, out_features=6, bias=True)\n",
      "    (12): ReLU()\n",
      "  )\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout2): Dropout(p=0.25, inplace=False)\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout3): Dropout(p=0.25, inplace=False)\n",
      "  (fc1): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=5120, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (parallel_layers): ModuleList(\n",
      "    (0-10): 11 x Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=37, bias=True)\n",
      "      (1): Softmax(dim=None)\n",
      "    )\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=90, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = md.CNN(init_weights=False)\n",
    "model.to(DEVICE)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "if Optimizer_type == 'ADAM' :\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr = Learning_rate, weight_decay = Weight_decay)\n",
    "elif Optimizer_type == 'SGD' :\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr = Learning_rate, weight_decay = Weight_decay)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "## Training function ##\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "  for batch_idx, (X,y) in enumerate(dataloader):\n",
    "\n",
    "    X = X.to(DEVICE)\n",
    "    pred = model(X)\n",
    "    loss = 0.\n",
    "    for idx in range(11):\n",
    "      y[idx] = y[idx].to(DEVICE)\n",
    "      loss += loss_fn(torch.tensor(pred[idx], dtype=torch.float32), y[idx])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.requires_grad_(True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_idx % 5 == 0:\n",
    "      print('loss: {:.6f} [{}/{}]'.format(loss.item(), batch_idx*len(X), len(dataloader.dataset)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "## Validation function ##\n",
    "def validation(model, valdata):\n",
    "  ## Input : trained model, validation data\n",
    "  ## Output : validation loss\n",
    "\n",
    "  model.eval()\n",
    "  val_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in valdata:\n",
    "      X = X.to(DEVICE)\n",
    "      pred = model(X)\n",
    "      for idx in range(11):\n",
    "        y[idx] = y[idx].to(DEVICE)\n",
    "        pred[idx] = pred[idx].squeeze()\n",
    "        val_loss += loss_fn(torch.tensor(torch.argmax(pred[idx], dim=-1), dtype=torch.float32), torch.tensor(y[idx], dtype=torch.float32))\n",
    "      pred = pred.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "  val_loss /= len(valdata.dataset)\n",
    "  print('\\n***Validation Result***\\nAverage loss: {:.6f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(val_loss, correct, len(valdata.dataset), 100*correct/len(valdata.dataset)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1]\n",
      "loss: 39.715469 [0/1000]\n",
      "loss: 39.751678 [20/1000]\n",
      "loss: 39.734108 [40/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wonjoon_LAB\\PycharmProjects\\AIGS538_Carplate_letter_sequence_geneneration\\venv\\lib\\site-packages\\torch\\nn\\functional.py:4358: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wonjoon_LAB\\PycharmProjects\\AIGS538_Carplate_letter_sequence_geneneration\\venv\\lib\\site-packages\\torch\\nn\\functional.py:4296: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Wonjoon_LAB\\PycharmProjects\\AIGS538_Carplate_letter_sequence_geneneration\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "C:\\Users\\Wonjoon_LAB\\AppData\\Local\\Temp\\ipykernel_7872\\1391650375.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss += loss_fn(torch.tensor(pred[idx], dtype=torch.float32), y[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 39.711449 [60/1000]\n",
      "loss: 39.731693 [80/1000]\n",
      "loss: 39.704437 [100/1000]\n",
      "loss: 39.728943 [120/1000]\n",
      "loss: 39.726768 [140/1000]\n",
      "loss: 39.730057 [160/1000]\n",
      "loss: 39.725296 [180/1000]\n",
      "loss: 39.747025 [200/1000]\n",
      "loss: 39.746658 [220/1000]\n",
      "loss: 39.736149 [240/1000]\n",
      "loss: 39.706173 [260/1000]\n",
      "loss: 39.691315 [280/1000]\n",
      "loss: 39.724220 [300/1000]\n",
      "loss: 39.725029 [320/1000]\n",
      "loss: 39.715622 [340/1000]\n",
      "loss: 39.711304 [360/1000]\n",
      "loss: 39.743328 [380/1000]\n",
      "loss: 39.734100 [400/1000]\n",
      "loss: 39.722099 [420/1000]\n",
      "loss: 39.732361 [440/1000]\n",
      "loss: 39.714367 [460/1000]\n",
      "loss: 39.736221 [480/1000]\n",
      "loss: 39.736687 [500/1000]\n",
      "loss: 39.736885 [520/1000]\n",
      "loss: 39.715935 [540/1000]\n",
      "loss: 39.721775 [560/1000]\n",
      "loss: 39.732445 [580/1000]\n",
      "loss: 39.728203 [600/1000]\n",
      "loss: 39.717705 [620/1000]\n",
      "loss: 39.718407 [640/1000]\n",
      "loss: 39.743904 [660/1000]\n",
      "loss: 39.702164 [680/1000]\n",
      "loss: 39.732151 [700/1000]\n",
      "loss: 39.689301 [720/1000]\n",
      "loss: 39.730026 [740/1000]\n",
      "loss: 39.719406 [760/1000]\n",
      "loss: 39.715527 [780/1000]\n",
      "loss: 39.707638 [800/1000]\n",
      "loss: 39.724464 [820/1000]\n",
      "loss: 39.700829 [840/1000]\n",
      "loss: 39.719646 [860/1000]\n",
      "loss: 39.722290 [880/1000]\n",
      "loss: 39.703384 [900/1000]\n",
      "loss: 39.708454 [920/1000]\n",
      "loss: 39.728859 [940/1000]\n",
      "loss: 39.732857 [960/1000]\n",
      "loss: 39.762264 [980/1000]\n",
      "\n",
      "[Epoch 2]\n",
      "loss: 39.742031 [0/1000]\n",
      "loss: 39.718109 [20/1000]\n",
      "loss: 39.738819 [40/1000]\n",
      "loss: 39.719814 [60/1000]\n",
      "loss: 39.721146 [80/1000]\n",
      "loss: 39.711002 [100/1000]\n",
      "loss: 39.694252 [120/1000]\n",
      "loss: 39.723969 [140/1000]\n",
      "loss: 39.722958 [160/1000]\n",
      "loss: 39.719513 [180/1000]\n",
      "loss: 39.722847 [200/1000]\n",
      "loss: 39.714413 [220/1000]\n",
      "loss: 39.730553 [240/1000]\n",
      "loss: 39.700199 [260/1000]\n",
      "loss: 39.736450 [280/1000]\n",
      "loss: 39.725307 [300/1000]\n",
      "loss: 39.731117 [320/1000]\n",
      "loss: 39.695599 [340/1000]\n",
      "loss: 39.716164 [360/1000]\n",
      "loss: 39.736710 [380/1000]\n",
      "loss: 39.733631 [400/1000]\n",
      "loss: 39.681473 [420/1000]\n",
      "loss: 39.731552 [440/1000]\n",
      "loss: 39.728882 [460/1000]\n",
      "loss: 39.730480 [480/1000]\n",
      "loss: 39.725899 [500/1000]\n",
      "loss: 39.725929 [520/1000]\n",
      "loss: 39.738331 [540/1000]\n",
      "loss: 39.740887 [560/1000]\n",
      "loss: 39.722996 [580/1000]\n",
      "loss: 39.707996 [600/1000]\n",
      "loss: 39.718044 [620/1000]\n",
      "loss: 39.717190 [640/1000]\n",
      "loss: 39.733757 [660/1000]\n",
      "loss: 39.727028 [680/1000]\n",
      "loss: 39.721676 [700/1000]\n",
      "loss: 39.746231 [720/1000]\n",
      "loss: 39.728039 [740/1000]\n",
      "loss: 39.732258 [760/1000]\n",
      "loss: 39.726330 [780/1000]\n",
      "loss: 39.735790 [800/1000]\n",
      "loss: 39.744728 [820/1000]\n",
      "loss: 39.713551 [840/1000]\n",
      "loss: 39.728386 [860/1000]\n",
      "loss: 39.723381 [880/1000]\n",
      "loss: 39.714180 [900/1000]\n",
      "loss: 39.714302 [920/1000]\n",
      "loss: 39.705338 [940/1000]\n",
      "loss: 39.739670 [960/1000]\n",
      "loss: 39.726257 [980/1000]\n",
      "\n",
      "[Epoch 3]\n",
      "loss: 39.747665 [0/1000]\n",
      "loss: 39.720322 [20/1000]\n",
      "loss: 39.728436 [40/1000]\n",
      "loss: 39.704903 [60/1000]\n",
      "loss: 39.742104 [80/1000]\n",
      "loss: 39.748653 [100/1000]\n",
      "loss: 39.704865 [120/1000]\n",
      "loss: 39.734283 [140/1000]\n",
      "loss: 39.702065 [160/1000]\n",
      "loss: 39.739677 [180/1000]\n",
      "loss: 39.716782 [200/1000]\n",
      "loss: 39.727898 [220/1000]\n",
      "loss: 39.720592 [240/1000]\n",
      "loss: 39.737587 [260/1000]\n",
      "loss: 39.723335 [280/1000]\n",
      "loss: 39.719807 [300/1000]\n",
      "loss: 39.741844 [320/1000]\n",
      "loss: 39.703541 [340/1000]\n",
      "loss: 39.715294 [360/1000]\n",
      "loss: 39.716347 [380/1000]\n",
      "loss: 39.736565 [400/1000]\n",
      "loss: 39.712711 [420/1000]\n",
      "loss: 39.701595 [440/1000]\n",
      "loss: 39.738396 [460/1000]\n",
      "loss: 39.741085 [480/1000]\n",
      "loss: 39.732719 [500/1000]\n",
      "loss: 39.749466 [520/1000]\n",
      "loss: 39.690475 [540/1000]\n",
      "loss: 39.725231 [560/1000]\n",
      "loss: 39.737026 [580/1000]\n",
      "loss: 39.727032 [600/1000]\n",
      "loss: 39.720757 [620/1000]\n",
      "loss: 39.743446 [640/1000]\n",
      "loss: 39.724518 [660/1000]\n",
      "loss: 39.729279 [680/1000]\n",
      "loss: 39.730877 [700/1000]\n",
      "loss: 39.752739 [720/1000]\n",
      "loss: 39.724773 [740/1000]\n",
      "loss: 39.719700 [760/1000]\n",
      "loss: 39.710018 [780/1000]\n",
      "loss: 39.732746 [800/1000]\n",
      "loss: 39.730553 [820/1000]\n",
      "loss: 39.720592 [840/1000]\n",
      "loss: 39.720219 [860/1000]\n",
      "loss: 39.751354 [880/1000]\n",
      "loss: 39.723122 [900/1000]\n",
      "loss: 39.705299 [920/1000]\n",
      "loss: 39.725365 [940/1000]\n",
      "loss: 39.744740 [960/1000]\n",
      "loss: 39.711132 [980/1000]\n",
      "\n",
      "[Epoch 4]\n",
      "loss: 39.744076 [0/1000]\n",
      "loss: 39.721153 [20/1000]\n",
      "loss: 39.717041 [40/1000]\n",
      "loss: 39.715950 [60/1000]\n",
      "loss: 39.723194 [80/1000]\n",
      "loss: 39.703560 [100/1000]\n",
      "loss: 39.715393 [120/1000]\n",
      "loss: 39.717979 [140/1000]\n",
      "loss: 39.731762 [160/1000]\n",
      "loss: 39.735218 [180/1000]\n",
      "loss: 39.728325 [200/1000]\n",
      "loss: 39.727020 [220/1000]\n",
      "loss: 39.708942 [240/1000]\n",
      "loss: 39.741268 [260/1000]\n",
      "loss: 39.706085 [280/1000]\n",
      "loss: 39.743408 [300/1000]\n",
      "loss: 39.716797 [320/1000]\n",
      "loss: 39.730976 [340/1000]\n",
      "loss: 39.727482 [360/1000]\n",
      "loss: 39.720383 [380/1000]\n",
      "loss: 39.725033 [400/1000]\n",
      "loss: 39.741470 [420/1000]\n",
      "loss: 39.743393 [440/1000]\n",
      "loss: 39.710888 [460/1000]\n",
      "loss: 39.706451 [480/1000]\n",
      "loss: 39.732681 [500/1000]\n",
      "loss: 39.718983 [520/1000]\n",
      "loss: 39.725845 [540/1000]\n",
      "loss: 39.718365 [560/1000]\n",
      "loss: 39.725582 [580/1000]\n",
      "loss: 39.730019 [600/1000]\n",
      "loss: 39.718800 [620/1000]\n",
      "loss: 39.713852 [640/1000]\n",
      "loss: 39.712078 [660/1000]\n",
      "loss: 39.732254 [680/1000]\n",
      "loss: 39.718922 [700/1000]\n",
      "loss: 39.750820 [720/1000]\n",
      "loss: 39.719097 [740/1000]\n",
      "loss: 39.714447 [760/1000]\n",
      "loss: 39.744110 [780/1000]\n",
      "loss: 39.700710 [800/1000]\n",
      "loss: 39.737373 [820/1000]\n",
      "loss: 39.740162 [840/1000]\n",
      "loss: 39.739941 [860/1000]\n",
      "loss: 39.746468 [880/1000]\n",
      "loss: 39.715992 [900/1000]\n",
      "loss: 39.723511 [920/1000]\n",
      "loss: 39.742985 [940/1000]\n",
      "loss: 39.718647 [960/1000]\n",
      "loss: 39.731609 [980/1000]\n",
      "\n",
      "[Epoch 5]\n",
      "loss: 39.706718 [0/1000]\n",
      "loss: 39.698475 [20/1000]\n",
      "loss: 39.721218 [40/1000]\n",
      "loss: 39.737850 [60/1000]\n",
      "loss: 39.715771 [80/1000]\n",
      "loss: 39.735527 [100/1000]\n",
      "loss: 39.732716 [120/1000]\n",
      "loss: 39.698235 [140/1000]\n",
      "loss: 39.722961 [160/1000]\n",
      "loss: 39.751549 [180/1000]\n",
      "loss: 39.721619 [200/1000]\n",
      "loss: 39.728882 [220/1000]\n",
      "loss: 39.735172 [240/1000]\n",
      "loss: 39.738544 [260/1000]\n",
      "loss: 39.722878 [280/1000]\n",
      "loss: 39.688305 [300/1000]\n",
      "loss: 39.710899 [320/1000]\n",
      "loss: 39.699009 [340/1000]\n",
      "loss: 39.730629 [360/1000]\n",
      "loss: 39.686932 [380/1000]\n",
      "loss: 39.714943 [400/1000]\n",
      "loss: 39.737503 [420/1000]\n",
      "loss: 39.728630 [440/1000]\n",
      "loss: 39.719585 [460/1000]\n",
      "loss: 39.729290 [480/1000]\n",
      "loss: 39.727661 [500/1000]\n",
      "loss: 39.723782 [520/1000]\n",
      "loss: 39.713932 [540/1000]\n",
      "loss: 39.702229 [560/1000]\n",
      "loss: 39.725151 [580/1000]\n",
      "loss: 39.721989 [600/1000]\n",
      "loss: 39.740658 [620/1000]\n",
      "loss: 39.737335 [640/1000]\n",
      "loss: 39.742615 [660/1000]\n",
      "loss: 39.734730 [680/1000]\n",
      "loss: 39.736317 [700/1000]\n",
      "loss: 39.706005 [720/1000]\n",
      "loss: 39.731529 [740/1000]\n",
      "loss: 39.718380 [760/1000]\n",
      "loss: 39.743759 [780/1000]\n",
      "loss: 39.728481 [800/1000]\n",
      "loss: 39.732830 [820/1000]\n",
      "loss: 39.711182 [840/1000]\n",
      "loss: 39.744411 [860/1000]\n",
      "loss: 39.735161 [880/1000]\n",
      "loss: 39.718643 [900/1000]\n",
      "loss: 39.709816 [920/1000]\n",
      "loss: 39.732666 [940/1000]\n",
      "loss: 39.713566 [960/1000]\n",
      "loss: 39.720665 [980/1000]\n",
      "\n",
      "[Epoch 6]\n",
      "loss: 39.722462 [0/1000]\n",
      "loss: 39.712017 [20/1000]\n",
      "loss: 39.725609 [40/1000]\n",
      "loss: 39.743416 [60/1000]\n",
      "loss: 39.715748 [80/1000]\n",
      "loss: 39.698853 [100/1000]\n",
      "loss: 39.720192 [120/1000]\n",
      "loss: 39.714478 [140/1000]\n",
      "loss: 39.721039 [160/1000]\n",
      "loss: 39.713329 [180/1000]\n",
      "loss: 39.732937 [200/1000]\n",
      "loss: 39.715729 [220/1000]\n",
      "loss: 39.729267 [240/1000]\n",
      "loss: 39.743824 [260/1000]\n",
      "loss: 39.734901 [280/1000]\n",
      "loss: 39.714439 [300/1000]\n",
      "loss: 39.715733 [320/1000]\n",
      "loss: 39.757393 [340/1000]\n",
      "loss: 39.715286 [360/1000]\n",
      "loss: 39.706581 [380/1000]\n",
      "loss: 39.693760 [400/1000]\n",
      "loss: 39.715286 [420/1000]\n",
      "loss: 39.736092 [440/1000]\n",
      "loss: 39.715408 [460/1000]\n",
      "loss: 39.715988 [480/1000]\n",
      "loss: 39.745567 [500/1000]\n",
      "loss: 39.749077 [520/1000]\n",
      "loss: 39.710999 [540/1000]\n",
      "loss: 39.722824 [560/1000]\n",
      "loss: 39.739971 [580/1000]\n",
      "loss: 39.711281 [600/1000]\n",
      "loss: 39.727249 [620/1000]\n",
      "loss: 39.728149 [640/1000]\n",
      "loss: 39.731365 [660/1000]\n",
      "loss: 39.703976 [680/1000]\n",
      "loss: 39.725750 [700/1000]\n",
      "loss: 39.735714 [720/1000]\n",
      "loss: 39.739559 [740/1000]\n",
      "loss: 39.731960 [760/1000]\n",
      "loss: 39.723457 [780/1000]\n",
      "loss: 39.714249 [800/1000]\n",
      "loss: 39.722591 [820/1000]\n",
      "loss: 39.735035 [840/1000]\n",
      "loss: 39.721382 [860/1000]\n",
      "loss: 39.706696 [880/1000]\n",
      "loss: 39.709267 [900/1000]\n",
      "loss: 39.746330 [920/1000]\n",
      "loss: 39.733704 [940/1000]\n",
      "loss: 39.734230 [960/1000]\n",
      "loss: 39.719032 [980/1000]\n",
      "\n",
      "[Epoch 7]\n",
      "loss: 39.745396 [0/1000]\n",
      "loss: 39.720562 [20/1000]\n",
      "loss: 39.723190 [40/1000]\n",
      "loss: 39.713474 [60/1000]\n",
      "loss: 39.723713 [80/1000]\n",
      "loss: 39.722923 [100/1000]\n",
      "loss: 39.725914 [120/1000]\n",
      "loss: 39.740456 [140/1000]\n",
      "loss: 39.718655 [160/1000]\n",
      "loss: 39.720577 [180/1000]\n",
      "loss: 39.706371 [200/1000]\n",
      "loss: 39.732059 [220/1000]\n",
      "loss: 39.716759 [240/1000]\n",
      "loss: 39.719189 [260/1000]\n",
      "loss: 39.724678 [280/1000]\n",
      "loss: 39.728352 [300/1000]\n",
      "loss: 39.702789 [320/1000]\n",
      "loss: 39.744713 [340/1000]\n",
      "loss: 39.723511 [360/1000]\n",
      "loss: 39.751408 [380/1000]\n",
      "loss: 39.734329 [400/1000]\n",
      "loss: 39.733532 [420/1000]\n",
      "loss: 39.735332 [440/1000]\n",
      "loss: 39.711037 [460/1000]\n",
      "loss: 39.721489 [480/1000]\n",
      "loss: 39.741489 [500/1000]\n",
      "loss: 39.715286 [520/1000]\n",
      "loss: 39.730774 [540/1000]\n",
      "loss: 39.704254 [560/1000]\n",
      "loss: 39.750351 [580/1000]\n",
      "loss: 39.743523 [600/1000]\n",
      "loss: 39.709820 [620/1000]\n",
      "loss: 39.733078 [640/1000]\n",
      "loss: 39.716244 [660/1000]\n",
      "loss: 39.736176 [680/1000]\n",
      "loss: 39.716564 [700/1000]\n",
      "loss: 39.724300 [720/1000]\n",
      "loss: 39.732384 [740/1000]\n",
      "loss: 39.713837 [760/1000]\n",
      "loss: 39.707157 [780/1000]\n",
      "loss: 39.718941 [800/1000]\n",
      "loss: 39.738453 [820/1000]\n",
      "loss: 39.729694 [840/1000]\n",
      "loss: 39.732830 [860/1000]\n",
      "loss: 39.741295 [880/1000]\n",
      "loss: 39.719852 [900/1000]\n",
      "loss: 39.716190 [920/1000]\n",
      "loss: 39.712425 [940/1000]\n",
      "loss: 39.725029 [960/1000]\n",
      "loss: 39.753456 [980/1000]\n",
      "\n",
      "[Epoch 8]\n",
      "loss: 39.727139 [0/1000]\n",
      "loss: 39.732018 [20/1000]\n",
      "loss: 39.726894 [40/1000]\n",
      "loss: 39.712608 [60/1000]\n",
      "loss: 39.722599 [80/1000]\n",
      "loss: 39.744251 [100/1000]\n",
      "loss: 39.710876 [120/1000]\n",
      "loss: 39.721230 [140/1000]\n",
      "loss: 39.741520 [160/1000]\n",
      "loss: 39.729149 [180/1000]\n",
      "loss: 39.730389 [200/1000]\n",
      "loss: 39.743866 [220/1000]\n",
      "loss: 39.740437 [240/1000]\n",
      "loss: 39.740341 [260/1000]\n",
      "loss: 39.723381 [280/1000]\n",
      "loss: 39.747414 [300/1000]\n",
      "loss: 39.739918 [320/1000]\n",
      "loss: 39.728432 [340/1000]\n",
      "loss: 39.737766 [360/1000]\n",
      "loss: 39.723686 [380/1000]\n",
      "loss: 39.722805 [400/1000]\n",
      "loss: 39.731945 [420/1000]\n",
      "loss: 39.712624 [440/1000]\n",
      "loss: 39.721203 [460/1000]\n",
      "loss: 39.729580 [480/1000]\n",
      "loss: 39.718975 [500/1000]\n",
      "loss: 39.743015 [520/1000]\n",
      "loss: 39.740723 [540/1000]\n",
      "loss: 39.742130 [560/1000]\n",
      "loss: 39.718159 [580/1000]\n",
      "loss: 39.723309 [600/1000]\n",
      "loss: 39.736652 [620/1000]\n",
      "loss: 39.716877 [640/1000]\n",
      "loss: 39.716503 [660/1000]\n",
      "loss: 39.724846 [680/1000]\n",
      "loss: 39.741440 [700/1000]\n",
      "loss: 39.734318 [720/1000]\n",
      "loss: 39.726559 [740/1000]\n",
      "loss: 39.716072 [760/1000]\n",
      "loss: 39.722824 [780/1000]\n",
      "loss: 39.717293 [800/1000]\n",
      "loss: 39.745071 [820/1000]\n",
      "loss: 39.737595 [840/1000]\n",
      "loss: 39.695454 [860/1000]\n",
      "loss: 39.729778 [880/1000]\n",
      "loss: 39.703564 [900/1000]\n",
      "loss: 39.745205 [920/1000]\n",
      "loss: 39.714447 [940/1000]\n",
      "loss: 39.723068 [960/1000]\n",
      "loss: 39.726982 [980/1000]\n",
      "\n",
      "[Epoch 9]\n",
      "loss: 39.739349 [0/1000]\n",
      "loss: 39.736710 [20/1000]\n",
      "loss: 39.728893 [40/1000]\n",
      "loss: 39.709045 [60/1000]\n",
      "loss: 39.715157 [80/1000]\n",
      "loss: 39.737736 [100/1000]\n",
      "loss: 39.730801 [120/1000]\n",
      "loss: 39.736248 [140/1000]\n",
      "loss: 39.738617 [160/1000]\n",
      "loss: 39.702065 [180/1000]\n",
      "loss: 39.712555 [200/1000]\n",
      "loss: 39.755146 [220/1000]\n",
      "loss: 39.725727 [240/1000]\n",
      "loss: 39.730198 [260/1000]\n",
      "loss: 39.742870 [280/1000]\n",
      "loss: 39.750946 [300/1000]\n",
      "loss: 39.718479 [320/1000]\n",
      "loss: 39.704861 [340/1000]\n",
      "loss: 39.712299 [360/1000]\n",
      "loss: 39.707844 [380/1000]\n",
      "loss: 39.698135 [400/1000]\n",
      "loss: 39.720665 [420/1000]\n",
      "loss: 39.718609 [440/1000]\n",
      "loss: 39.734394 [460/1000]\n",
      "loss: 39.743092 [480/1000]\n",
      "loss: 39.728004 [500/1000]\n",
      "loss: 39.731319 [520/1000]\n",
      "loss: 39.705826 [540/1000]\n",
      "loss: 39.751999 [560/1000]\n",
      "loss: 39.729168 [580/1000]\n",
      "loss: 39.708809 [600/1000]\n",
      "loss: 39.722675 [620/1000]\n",
      "loss: 39.742825 [640/1000]\n",
      "loss: 39.723984 [660/1000]\n",
      "loss: 39.743156 [680/1000]\n",
      "loss: 39.733986 [700/1000]\n",
      "loss: 39.733494 [720/1000]\n",
      "loss: 39.724701 [740/1000]\n",
      "loss: 39.743793 [760/1000]\n",
      "loss: 39.727100 [780/1000]\n",
      "loss: 39.738289 [800/1000]\n",
      "loss: 39.727222 [820/1000]\n",
      "loss: 39.743397 [840/1000]\n",
      "loss: 39.708885 [860/1000]\n",
      "loss: 39.737514 [880/1000]\n",
      "loss: 39.732357 [900/1000]\n",
      "loss: 39.753506 [920/1000]\n",
      "loss: 39.741676 [940/1000]\n",
      "loss: 39.742096 [960/1000]\n",
      "loss: 39.746338 [980/1000]\n",
      "\n",
      "[Epoch 10]\n",
      "loss: 39.728397 [0/1000]\n",
      "loss: 39.711590 [20/1000]\n",
      "loss: 39.750179 [40/1000]\n",
      "loss: 39.721134 [60/1000]\n",
      "loss: 39.718117 [80/1000]\n",
      "loss: 39.724106 [100/1000]\n",
      "loss: 39.712486 [120/1000]\n",
      "loss: 39.737438 [140/1000]\n",
      "loss: 39.722729 [160/1000]\n",
      "loss: 39.746799 [180/1000]\n",
      "loss: 39.731651 [200/1000]\n",
      "loss: 39.743156 [220/1000]\n",
      "loss: 39.713249 [240/1000]\n",
      "loss: 39.732979 [260/1000]\n",
      "loss: 39.736687 [280/1000]\n",
      "loss: 39.741474 [300/1000]\n",
      "loss: 39.725365 [320/1000]\n",
      "loss: 39.722595 [340/1000]\n",
      "loss: 39.707169 [360/1000]\n",
      "loss: 39.711929 [380/1000]\n",
      "loss: 39.743191 [400/1000]\n",
      "loss: 39.728462 [420/1000]\n",
      "loss: 39.722160 [440/1000]\n",
      "loss: 39.714111 [460/1000]\n",
      "loss: 39.726032 [480/1000]\n",
      "loss: 39.721428 [500/1000]\n",
      "loss: 39.705982 [520/1000]\n",
      "loss: 39.714386 [540/1000]\n",
      "loss: 39.725460 [560/1000]\n",
      "loss: 39.720581 [580/1000]\n",
      "loss: 39.730568 [600/1000]\n",
      "loss: 39.731110 [620/1000]\n",
      "loss: 39.711403 [640/1000]\n",
      "loss: 39.727634 [660/1000]\n",
      "loss: 39.743130 [680/1000]\n",
      "loss: 39.717243 [700/1000]\n",
      "loss: 39.716553 [720/1000]\n",
      "loss: 39.722511 [740/1000]\n",
      "loss: 39.723976 [760/1000]\n",
      "loss: 39.735188 [780/1000]\n",
      "loss: 39.734131 [800/1000]\n",
      "loss: 39.732548 [820/1000]\n",
      "loss: 39.733269 [840/1000]\n",
      "loss: 39.702374 [860/1000]\n",
      "loss: 39.711563 [880/1000]\n",
      "loss: 39.732872 [900/1000]\n",
      "loss: 39.662468 [920/1000]\n",
      "loss: 39.726273 [940/1000]\n",
      "loss: 39.732193 [960/1000]\n",
      "loss: 39.706158 [980/1000]\n",
      "\n",
      "[Epoch 11]\n",
      "loss: 39.725285 [0/1000]\n",
      "loss: 39.715065 [20/1000]\n",
      "loss: 39.723839 [40/1000]\n",
      "loss: 39.736313 [60/1000]\n",
      "loss: 39.720222 [80/1000]\n",
      "loss: 39.717186 [100/1000]\n",
      "loss: 39.721546 [120/1000]\n",
      "loss: 39.724651 [140/1000]\n",
      "loss: 39.714684 [160/1000]\n",
      "loss: 39.719582 [180/1000]\n",
      "loss: 39.733021 [200/1000]\n",
      "loss: 39.729824 [220/1000]\n",
      "loss: 39.723446 [240/1000]\n",
      "loss: 39.729767 [260/1000]\n",
      "loss: 39.733994 [280/1000]\n",
      "loss: 39.702599 [300/1000]\n",
      "loss: 39.713249 [320/1000]\n",
      "loss: 39.736805 [340/1000]\n",
      "loss: 39.719322 [360/1000]\n",
      "loss: 39.708664 [380/1000]\n",
      "loss: 39.739300 [400/1000]\n",
      "loss: 39.740192 [420/1000]\n",
      "loss: 39.737675 [440/1000]\n",
      "loss: 39.733608 [460/1000]\n",
      "loss: 39.700733 [480/1000]\n",
      "loss: 39.751286 [500/1000]\n",
      "loss: 39.744663 [520/1000]\n",
      "loss: 39.733677 [540/1000]\n",
      "loss: 39.746132 [560/1000]\n",
      "loss: 39.720184 [580/1000]\n",
      "loss: 39.731499 [600/1000]\n",
      "loss: 39.740166 [620/1000]\n",
      "loss: 39.730675 [640/1000]\n",
      "loss: 39.723736 [660/1000]\n",
      "loss: 39.705727 [680/1000]\n",
      "loss: 39.723801 [700/1000]\n",
      "loss: 39.714928 [720/1000]\n",
      "loss: 39.740952 [740/1000]\n",
      "loss: 39.704659 [760/1000]\n",
      "loss: 39.699249 [780/1000]\n",
      "loss: 39.732399 [800/1000]\n",
      "loss: 39.758324 [820/1000]\n",
      "loss: 39.746510 [840/1000]\n",
      "loss: 39.730030 [860/1000]\n",
      "loss: 39.716183 [880/1000]\n",
      "loss: 39.717506 [900/1000]\n",
      "loss: 39.735550 [920/1000]\n",
      "loss: 39.740040 [940/1000]\n",
      "loss: 39.731266 [960/1000]\n",
      "loss: 39.727249 [980/1000]\n",
      "\n",
      "[Epoch 12]\n",
      "loss: 39.742493 [0/1000]\n",
      "loss: 39.721142 [20/1000]\n",
      "loss: 39.735329 [40/1000]\n",
      "loss: 39.714466 [60/1000]\n",
      "loss: 39.724205 [80/1000]\n",
      "loss: 39.741276 [100/1000]\n",
      "loss: 39.710243 [120/1000]\n",
      "loss: 39.725998 [140/1000]\n",
      "loss: 39.736675 [160/1000]\n",
      "loss: 39.739262 [180/1000]\n",
      "loss: 39.719673 [200/1000]\n",
      "loss: 39.722149 [220/1000]\n",
      "loss: 39.730537 [240/1000]\n",
      "loss: 39.715607 [260/1000]\n",
      "loss: 39.733257 [280/1000]\n",
      "loss: 39.740227 [300/1000]\n",
      "loss: 39.754669 [320/1000]\n",
      "loss: 39.724663 [340/1000]\n",
      "loss: 39.726692 [360/1000]\n",
      "loss: 39.733719 [380/1000]\n",
      "loss: 39.714893 [400/1000]\n",
      "loss: 39.735500 [420/1000]\n",
      "loss: 39.725739 [440/1000]\n",
      "loss: 39.727310 [460/1000]\n",
      "loss: 39.765419 [480/1000]\n",
      "loss: 39.738216 [500/1000]\n",
      "loss: 39.715752 [520/1000]\n",
      "loss: 39.736008 [540/1000]\n",
      "loss: 39.712784 [560/1000]\n",
      "loss: 39.725445 [580/1000]\n",
      "loss: 39.733021 [600/1000]\n",
      "loss: 39.727142 [620/1000]\n",
      "loss: 39.685951 [640/1000]\n",
      "loss: 39.732128 [660/1000]\n",
      "loss: 39.723030 [680/1000]\n",
      "loss: 39.720036 [700/1000]\n",
      "loss: 39.706306 [720/1000]\n",
      "loss: 39.738075 [740/1000]\n",
      "loss: 39.728386 [760/1000]\n",
      "loss: 39.720959 [780/1000]\n",
      "loss: 39.725960 [800/1000]\n",
      "loss: 39.747704 [820/1000]\n",
      "loss: 39.705696 [840/1000]\n",
      "loss: 39.734261 [860/1000]\n",
      "loss: 39.714005 [880/1000]\n",
      "loss: 39.743320 [900/1000]\n",
      "loss: 39.734638 [920/1000]\n",
      "loss: 39.710026 [940/1000]\n",
      "loss: 39.699959 [960/1000]\n",
      "loss: 39.747131 [980/1000]\n",
      "\n",
      "[Epoch 13]\n",
      "loss: 39.732727 [0/1000]\n",
      "loss: 39.727531 [20/1000]\n",
      "loss: 39.723949 [40/1000]\n",
      "loss: 39.729580 [60/1000]\n",
      "loss: 39.730606 [80/1000]\n",
      "loss: 39.720703 [100/1000]\n",
      "loss: 39.734665 [120/1000]\n",
      "loss: 39.739536 [140/1000]\n",
      "loss: 39.743458 [160/1000]\n",
      "loss: 39.731544 [180/1000]\n",
      "loss: 39.730812 [200/1000]\n",
      "loss: 39.749641 [220/1000]\n",
      "loss: 39.734001 [240/1000]\n",
      "loss: 39.741741 [260/1000]\n",
      "loss: 39.730804 [280/1000]\n",
      "loss: 39.739174 [300/1000]\n",
      "loss: 39.725544 [320/1000]\n",
      "loss: 39.716640 [340/1000]\n",
      "loss: 39.718285 [360/1000]\n",
      "loss: 39.721062 [380/1000]\n",
      "loss: 39.734329 [400/1000]\n",
      "loss: 39.728146 [420/1000]\n",
      "loss: 39.753235 [440/1000]\n",
      "loss: 39.734875 [460/1000]\n",
      "loss: 39.729343 [480/1000]\n",
      "loss: 39.741669 [500/1000]\n",
      "loss: 39.713779 [520/1000]\n",
      "loss: 39.748341 [540/1000]\n",
      "loss: 39.724262 [560/1000]\n",
      "loss: 39.742886 [580/1000]\n",
      "loss: 39.725460 [600/1000]\n",
      "loss: 39.716167 [620/1000]\n",
      "loss: 39.718842 [640/1000]\n",
      "loss: 39.746147 [660/1000]\n",
      "loss: 39.737965 [680/1000]\n",
      "loss: 39.722233 [700/1000]\n",
      "loss: 39.742413 [720/1000]\n",
      "loss: 39.747990 [740/1000]\n",
      "loss: 39.727249 [760/1000]\n",
      "loss: 39.745975 [780/1000]\n",
      "loss: 39.709854 [800/1000]\n",
      "loss: 39.735043 [820/1000]\n",
      "loss: 39.737156 [840/1000]\n",
      "loss: 39.733601 [860/1000]\n",
      "loss: 39.747265 [880/1000]\n",
      "loss: 39.718510 [900/1000]\n",
      "loss: 39.744419 [920/1000]\n",
      "loss: 39.731857 [940/1000]\n",
      "loss: 39.699482 [960/1000]\n",
      "loss: 39.736382 [980/1000]\n",
      "\n",
      "[Epoch 14]\n",
      "loss: 39.715576 [0/1000]\n",
      "loss: 39.731129 [20/1000]\n",
      "loss: 39.727512 [40/1000]\n",
      "loss: 39.711452 [60/1000]\n",
      "loss: 39.723988 [80/1000]\n",
      "loss: 39.743176 [100/1000]\n",
      "loss: 39.727890 [120/1000]\n",
      "loss: 39.699451 [140/1000]\n",
      "loss: 39.711655 [160/1000]\n",
      "loss: 39.712723 [180/1000]\n",
      "loss: 39.718525 [200/1000]\n",
      "loss: 39.736877 [220/1000]\n",
      "loss: 39.739609 [240/1000]\n",
      "loss: 39.710094 [260/1000]\n",
      "loss: 39.724228 [280/1000]\n",
      "loss: 39.744434 [300/1000]\n",
      "loss: 39.723305 [320/1000]\n",
      "loss: 39.745560 [340/1000]\n",
      "loss: 39.725170 [360/1000]\n",
      "loss: 39.712933 [380/1000]\n",
      "loss: 39.724049 [400/1000]\n",
      "loss: 39.707153 [420/1000]\n",
      "loss: 39.744530 [440/1000]\n",
      "loss: 39.728989 [460/1000]\n",
      "loss: 39.734379 [480/1000]\n",
      "loss: 39.726959 [500/1000]\n",
      "loss: 39.744419 [520/1000]\n",
      "loss: 39.728249 [540/1000]\n",
      "loss: 39.738083 [560/1000]\n",
      "loss: 39.712021 [580/1000]\n",
      "loss: 39.742073 [600/1000]\n",
      "loss: 39.729561 [620/1000]\n",
      "loss: 39.738056 [640/1000]\n",
      "loss: 39.734554 [660/1000]\n",
      "loss: 39.710453 [680/1000]\n",
      "loss: 39.729565 [700/1000]\n",
      "loss: 39.717251 [720/1000]\n",
      "loss: 39.712517 [740/1000]\n",
      "loss: 39.713840 [760/1000]\n",
      "loss: 39.731483 [780/1000]\n",
      "loss: 39.745426 [800/1000]\n",
      "loss: 39.730534 [820/1000]\n",
      "loss: 39.743214 [840/1000]\n",
      "loss: 39.683037 [860/1000]\n",
      "loss: 39.730293 [880/1000]\n",
      "loss: 39.738350 [900/1000]\n",
      "loss: 39.713341 [920/1000]\n",
      "loss: 39.719116 [940/1000]\n",
      "loss: 39.724270 [960/1000]\n",
      "loss: 39.749805 [980/1000]\n",
      "\n",
      "[Epoch 15]\n",
      "loss: 39.713215 [0/1000]\n",
      "loss: 39.714748 [20/1000]\n",
      "loss: 39.724625 [40/1000]\n",
      "loss: 39.716145 [60/1000]\n",
      "loss: 39.725662 [80/1000]\n",
      "loss: 39.732426 [100/1000]\n",
      "loss: 39.712719 [120/1000]\n",
      "loss: 39.749634 [140/1000]\n",
      "loss: 39.718609 [160/1000]\n",
      "loss: 39.731754 [180/1000]\n",
      "loss: 39.740932 [200/1000]\n",
      "loss: 39.689960 [220/1000]\n",
      "loss: 39.744961 [240/1000]\n",
      "loss: 39.722466 [260/1000]\n",
      "loss: 39.736088 [280/1000]\n",
      "loss: 39.713856 [300/1000]\n",
      "loss: 39.739113 [320/1000]\n",
      "loss: 39.747620 [340/1000]\n",
      "loss: 39.732452 [360/1000]\n",
      "loss: 39.721066 [380/1000]\n",
      "loss: 39.719528 [400/1000]\n",
      "loss: 39.742218 [420/1000]\n",
      "loss: 39.754551 [440/1000]\n",
      "loss: 39.730797 [460/1000]\n",
      "loss: 39.739395 [480/1000]\n",
      "loss: 39.733032 [500/1000]\n",
      "loss: 39.751595 [520/1000]\n",
      "loss: 39.704437 [540/1000]\n",
      "loss: 39.723930 [560/1000]\n",
      "loss: 39.731201 [580/1000]\n",
      "loss: 39.747845 [600/1000]\n",
      "loss: 39.739956 [620/1000]\n",
      "loss: 39.717484 [640/1000]\n",
      "loss: 39.722836 [660/1000]\n",
      "loss: 39.728714 [680/1000]\n",
      "loss: 39.719330 [700/1000]\n",
      "loss: 39.739456 [720/1000]\n",
      "loss: 39.719742 [740/1000]\n",
      "loss: 39.724838 [760/1000]\n",
      "loss: 39.727180 [780/1000]\n",
      "loss: 39.759293 [800/1000]\n",
      "loss: 39.718594 [820/1000]\n",
      "loss: 39.718590 [840/1000]\n",
      "loss: 39.717556 [860/1000]\n",
      "loss: 39.721657 [880/1000]\n",
      "loss: 39.727440 [900/1000]\n",
      "loss: 39.710777 [920/1000]\n",
      "loss: 39.734367 [940/1000]\n",
      "loss: 39.733429 [960/1000]\n",
      "loss: 39.724815 [980/1000]\n",
      "\n",
      "Done!\n",
      "Training time: 55.9556\n"
     ]
    }
   ],
   "source": [
    "## Training Operation ##\n",
    "start = time.time()\n",
    "for t in range(epochs):\n",
    "  print(f\"\\n[Epoch {t+1}]\")\n",
    "  train(train_loader, model, loss_fn, optimizer)\n",
    "  # validation(model, val_loader)\n",
    "print(\"\\nDone!\")\n",
    "end = time.time()\n",
    "\n",
    "print(\"Training time: {:.4f}\".format(end-start))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
